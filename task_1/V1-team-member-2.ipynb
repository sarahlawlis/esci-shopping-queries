{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_path = os.path.join('.', 'esci-shopping-queries/data', 'shopping_queries_dataset_examples.parquet')\n",
    "products_path = os.path.join('.', 'esci-shopping-queries/data', 'shopping_queries_dataset_products.parquet')\n",
    "sources_path = os.path.join('.', 'esci-shopping-queries/data', 'shopping_queries_dataset_sources.csv')\n",
    "\n",
    "examples = dd.read_parquet(examples_path)\n",
    "products = dd.read_parquet(products_path)\n",
    "sources = dd.read_csv(sources_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_products = dd.merge(\n",
    "    examples,\n",
    "    products,\n",
    "    how='left',\n",
    "    left_on=['product_locale','product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "examples_products = examples_products[examples_products['product_locale'] == 'us']\n",
    "\n",
    "task_2 = examples_products[examples_products['large_version'] == 1]\n",
    "\n",
    "# another thing that I changed \n",
    "# encoding the esci labels \n",
    "label_mapping = {'E': 0, \n",
    "                 'S': 1, \n",
    "                 'C': 2, \n",
    "                 'I': 3}\n",
    "\n",
    "task_2['encoded_labels'] = task_2['esci_label'].map(label_mapping).astype(int)\n",
    "\n",
    "\n",
    "task_2_train = task_2[task_2['split'] == 'train']\n",
    "task_2_test = task_2[task_2['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "model = AutoModel.from_pretrained('distilroberta-base').to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    batch_size = 16  # Adjust this size\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # adjusting this for max pooling \n",
    "        batch_embeddings, _ = torch.max(outputs.last_hidden_state, dim=1)\n",
    "        batch_embeddings = batch_embeddings.cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def process_partition(partition):\n",
    "    query_embeddings = generate_embeddings(partition['query'])\n",
    "    product_title_embeddings = generate_embeddings(partition['product_title'])\n",
    "\n",
    "    combined = torch.cat((torch.tensor(query_embeddings), torch.tensor(product_title_embeddings)), dim=1).numpy()\n",
    "    \n",
    "    print(f'Combined shape: {combined.shape}')  # Expecting (n, 1536)\n",
    "\n",
    "    result = pd.DataFrame(combined, index=partition.index, columns=[f'embedding_{i}' for i in range(combined.shape[1])])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame and enerate column names\n",
    "meta = pd.DataFrame(columns=[f'embedding_{i}' for i in range(2 * 768)], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the total number of rows in the df\n",
    "total_rows = task_2_train.shape[0].compute()\n",
    "\n",
    "# calculates the fraction of rows needed to sample 10000 \n",
    "sample_fraction = 10000 / total_rows\n",
    "\n",
    "# samples a fraction of the df \n",
    "task_2_train_sample = task_2_train.sample(frac=sample_fraction, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>small_version</th>\n",
       "      <th>large_version</th>\n",
       "      <th>split</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color</th>\n",
       "      <th>encoded_labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: sample, 17 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              example_id   query query_id product_id product_locale esci_label small_version large_version   split product_title product_description product_bullet_point product_brand product_color encoded_labels\n",
       "npartitions=1                                                                                                                                                                                                       \n",
       "                   int64  string    int64     string         string     string         int64         int64  string        string              string               string        string        string          int32\n",
       "                     ...     ...      ...        ...            ...        ...           ...           ...     ...           ...                 ...                  ...           ...           ...            ...\n",
       "Dask Name: sample, 17 graph layers"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_2_train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = task_2_train_sample.map_partitions(process_partition, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (10000, 1536)\n"
     ]
    }
   ],
   "source": [
    "result = result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_1526</th>\n",
       "      <th>embedding_1527</th>\n",
       "      <th>embedding_1528</th>\n",
       "      <th>embedding_1529</th>\n",
       "      <th>embedding_1530</th>\n",
       "      <th>embedding_1531</th>\n",
       "      <th>embedding_1532</th>\n",
       "      <th>embedding_1533</th>\n",
       "      <th>embedding_1534</th>\n",
       "      <th>embedding_1535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1322108</th>\n",
       "      <td>0.108705</td>\n",
       "      <td>0.234877</td>\n",
       "      <td>0.139450</td>\n",
       "      <td>0.315206</td>\n",
       "      <td>0.575190</td>\n",
       "      <td>-0.020448</td>\n",
       "      <td>0.148328</td>\n",
       "      <td>0.270037</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.128139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182917</td>\n",
       "      <td>0.098517</td>\n",
       "      <td>0.072425</td>\n",
       "      <td>0.049298</td>\n",
       "      <td>0.214076</td>\n",
       "      <td>0.081951</td>\n",
       "      <td>0.728155</td>\n",
       "      <td>0.395790</td>\n",
       "      <td>0.097055</td>\n",
       "      <td>0.106109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686437</th>\n",
       "      <td>0.128653</td>\n",
       "      <td>0.085390</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.089813</td>\n",
       "      <td>0.085738</td>\n",
       "      <td>-0.105572</td>\n",
       "      <td>0.078860</td>\n",
       "      <td>0.257313</td>\n",
       "      <td>0.048456</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269261</td>\n",
       "      <td>0.137719</td>\n",
       "      <td>0.355748</td>\n",
       "      <td>0.081922</td>\n",
       "      <td>0.274871</td>\n",
       "      <td>0.236418</td>\n",
       "      <td>0.301538</td>\n",
       "      <td>0.465826</td>\n",
       "      <td>0.222930</td>\n",
       "      <td>0.247738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135583</th>\n",
       "      <td>0.111750</td>\n",
       "      <td>0.206983</td>\n",
       "      <td>0.077643</td>\n",
       "      <td>-0.041382</td>\n",
       "      <td>0.817184</td>\n",
       "      <td>0.118343</td>\n",
       "      <td>0.085018</td>\n",
       "      <td>0.270429</td>\n",
       "      <td>0.149541</td>\n",
       "      <td>0.193659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307988</td>\n",
       "      <td>0.080208</td>\n",
       "      <td>0.200823</td>\n",
       "      <td>0.147451</td>\n",
       "      <td>0.306865</td>\n",
       "      <td>0.176306</td>\n",
       "      <td>0.818601</td>\n",
       "      <td>0.228154</td>\n",
       "      <td>0.178096</td>\n",
       "      <td>0.188407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566068</th>\n",
       "      <td>0.042544</td>\n",
       "      <td>0.328948</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.084752</td>\n",
       "      <td>1.112144</td>\n",
       "      <td>-0.033281</td>\n",
       "      <td>0.045591</td>\n",
       "      <td>0.149777</td>\n",
       "      <td>0.165218</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365519</td>\n",
       "      <td>0.049413</td>\n",
       "      <td>0.259212</td>\n",
       "      <td>0.177283</td>\n",
       "      <td>0.178866</td>\n",
       "      <td>0.166594</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.622622</td>\n",
       "      <td>0.218156</td>\n",
       "      <td>0.154736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075274</th>\n",
       "      <td>0.111251</td>\n",
       "      <td>0.145150</td>\n",
       "      <td>0.067874</td>\n",
       "      <td>0.081204</td>\n",
       "      <td>0.890214</td>\n",
       "      <td>-0.038702</td>\n",
       "      <td>0.101804</td>\n",
       "      <td>0.204692</td>\n",
       "      <td>0.066338</td>\n",
       "      <td>-0.024227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086346</td>\n",
       "      <td>0.180397</td>\n",
       "      <td>0.096318</td>\n",
       "      <td>0.155747</td>\n",
       "      <td>0.262609</td>\n",
       "      <td>0.157427</td>\n",
       "      <td>0.296501</td>\n",
       "      <td>0.315046</td>\n",
       "      <td>0.140594</td>\n",
       "      <td>0.244535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656352</th>\n",
       "      <td>0.238125</td>\n",
       "      <td>0.185478</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>0.261466</td>\n",
       "      <td>0.728569</td>\n",
       "      <td>-0.099423</td>\n",
       "      <td>0.199211</td>\n",
       "      <td>0.144787</td>\n",
       "      <td>0.068051</td>\n",
       "      <td>0.287547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617471</td>\n",
       "      <td>0.175846</td>\n",
       "      <td>0.573133</td>\n",
       "      <td>0.403009</td>\n",
       "      <td>0.477426</td>\n",
       "      <td>0.282736</td>\n",
       "      <td>0.492554</td>\n",
       "      <td>0.791985</td>\n",
       "      <td>0.324375</td>\n",
       "      <td>0.293089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603069</th>\n",
       "      <td>0.063194</td>\n",
       "      <td>0.217883</td>\n",
       "      <td>0.141870</td>\n",
       "      <td>0.170465</td>\n",
       "      <td>0.655925</td>\n",
       "      <td>-0.002001</td>\n",
       "      <td>0.123496</td>\n",
       "      <td>0.135748</td>\n",
       "      <td>0.042142</td>\n",
       "      <td>0.233673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195427</td>\n",
       "      <td>0.117208</td>\n",
       "      <td>0.421813</td>\n",
       "      <td>0.292528</td>\n",
       "      <td>0.437827</td>\n",
       "      <td>0.301911</td>\n",
       "      <td>0.611399</td>\n",
       "      <td>0.698749</td>\n",
       "      <td>0.273277</td>\n",
       "      <td>0.311094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815523</th>\n",
       "      <td>0.202807</td>\n",
       "      <td>0.227380</td>\n",
       "      <td>0.123965</td>\n",
       "      <td>0.099034</td>\n",
       "      <td>1.005392</td>\n",
       "      <td>0.480051</td>\n",
       "      <td>0.136926</td>\n",
       "      <td>0.050289</td>\n",
       "      <td>0.104759</td>\n",
       "      <td>0.291602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215847</td>\n",
       "      <td>0.118625</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.085508</td>\n",
       "      <td>0.313142</td>\n",
       "      <td>0.163584</td>\n",
       "      <td>0.459325</td>\n",
       "      <td>0.330234</td>\n",
       "      <td>0.162431</td>\n",
       "      <td>0.273381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732044</th>\n",
       "      <td>0.020294</td>\n",
       "      <td>0.086303</td>\n",
       "      <td>0.073896</td>\n",
       "      <td>0.338815</td>\n",
       "      <td>0.428603</td>\n",
       "      <td>0.239287</td>\n",
       "      <td>-0.016330</td>\n",
       "      <td>0.142109</td>\n",
       "      <td>0.039532</td>\n",
       "      <td>0.023233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202253</td>\n",
       "      <td>0.121261</td>\n",
       "      <td>0.296222</td>\n",
       "      <td>0.341769</td>\n",
       "      <td>0.307535</td>\n",
       "      <td>0.098423</td>\n",
       "      <td>0.500768</td>\n",
       "      <td>0.408014</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.286709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627562</th>\n",
       "      <td>0.106181</td>\n",
       "      <td>0.233717</td>\n",
       "      <td>0.106551</td>\n",
       "      <td>0.098795</td>\n",
       "      <td>0.503654</td>\n",
       "      <td>-0.022495</td>\n",
       "      <td>0.112938</td>\n",
       "      <td>0.054043</td>\n",
       "      <td>0.070425</td>\n",
       "      <td>0.099698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258592</td>\n",
       "      <td>0.237036</td>\n",
       "      <td>0.398716</td>\n",
       "      <td>0.132692</td>\n",
       "      <td>0.486762</td>\n",
       "      <td>0.192465</td>\n",
       "      <td>0.377412</td>\n",
       "      <td>0.606698</td>\n",
       "      <td>0.335490</td>\n",
       "      <td>0.408242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1536 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "1322108     0.108705     0.234877     0.139450     0.315206     0.575190   \n",
       "686437      0.128653     0.085390     0.074830     0.089813     0.085738   \n",
       "2135583     0.111750     0.206983     0.077643    -0.041382     0.817184   \n",
       "1566068     0.042544     0.328948     0.044401     0.084752     1.112144   \n",
       "2075274     0.111251     0.145150     0.067874     0.081204     0.890214   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "1656352     0.238125     0.185478     0.054519     0.261466     0.728569   \n",
       "603069      0.063194     0.217883     0.141870     0.170465     0.655925   \n",
       "1815523     0.202807     0.227380     0.123965     0.099034     1.005392   \n",
       "1732044     0.020294     0.086303     0.073896     0.338815     0.428603   \n",
       "627562      0.106181     0.233717     0.106551     0.098795     0.503654   \n",
       "\n",
       "         embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "1322108    -0.020448     0.148328     0.270037     0.075255     0.128139  ...   \n",
       "686437     -0.105572     0.078860     0.257313     0.048456    -0.021896  ...   \n",
       "2135583     0.118343     0.085018     0.270429     0.149541     0.193659  ...   \n",
       "1566068    -0.033281     0.045591     0.149777     0.165218     0.013646  ...   \n",
       "2075274    -0.038702     0.101804     0.204692     0.066338    -0.024227  ...   \n",
       "...              ...          ...          ...          ...          ...  ...   \n",
       "1656352    -0.099423     0.199211     0.144787     0.068051     0.287547  ...   \n",
       "603069     -0.002001     0.123496     0.135748     0.042142     0.233673  ...   \n",
       "1815523     0.480051     0.136926     0.050289     0.104759     0.291602  ...   \n",
       "1732044     0.239287    -0.016330     0.142109     0.039532     0.023233  ...   \n",
       "627562     -0.022495     0.112938     0.054043     0.070425     0.099698  ...   \n",
       "\n",
       "         embedding_1526  embedding_1527  embedding_1528  embedding_1529  \\\n",
       "1322108        0.182917        0.098517        0.072425        0.049298   \n",
       "686437         0.269261        0.137719        0.355748        0.081922   \n",
       "2135583        0.307988        0.080208        0.200823        0.147451   \n",
       "1566068        0.365519        0.049413        0.259212        0.177283   \n",
       "2075274        0.086346        0.180397        0.096318        0.155747   \n",
       "...                 ...             ...             ...             ...   \n",
       "1656352        0.617471        0.175846        0.573133        0.403009   \n",
       "603069         0.195427        0.117208        0.421813        0.292528   \n",
       "1815523        0.215847        0.118625        0.004812        0.085508   \n",
       "1732044        0.202253        0.121261        0.296222        0.341769   \n",
       "627562         0.258592        0.237036        0.398716        0.132692   \n",
       "\n",
       "         embedding_1530  embedding_1531  embedding_1532  embedding_1533  \\\n",
       "1322108        0.214076        0.081951        0.728155        0.395790   \n",
       "686437         0.274871        0.236418        0.301538        0.465826   \n",
       "2135583        0.306865        0.176306        0.818601        0.228154   \n",
       "1566068        0.178866        0.166594        0.643011        0.622622   \n",
       "2075274        0.262609        0.157427        0.296501        0.315046   \n",
       "...                 ...             ...             ...             ...   \n",
       "1656352        0.477426        0.282736        0.492554        0.791985   \n",
       "603069         0.437827        0.301911        0.611399        0.698749   \n",
       "1815523        0.313142        0.163584        0.459325        0.330234   \n",
       "1732044        0.307535        0.098423        0.500768        0.408014   \n",
       "627562         0.486762        0.192465        0.377412        0.606698   \n",
       "\n",
       "         embedding_1534  embedding_1535  \n",
       "1322108        0.097055        0.106109  \n",
       "686437         0.222930        0.247738  \n",
       "2135583        0.178096        0.188407  \n",
       "1566068        0.218156        0.154736  \n",
       "2075274        0.140594        0.244535  \n",
       "...                 ...             ...  \n",
       "1656352        0.324375        0.293089  \n",
       "603069         0.273277        0.311094  \n",
       "1815523        0.162431        0.273381  \n",
       "1732044        0.281700        0.286709  \n",
       "627562         0.335490        0.408242  \n",
       "\n",
       "[10000 rows x 1536 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Multi-Layer Preceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the Multi-Layer Preceptron model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  \n",
    "            nn.Dropout(0.1),  \n",
    "            nn.Linear(hidden_size, num_classes)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputting the parameters\n",
    "\n",
    "# the size of the concatenated embeddings(768 + 768)\n",
    "input_size = 1536  \n",
    "hidden_size = 128\n",
    "# number of classes Exact, Substitute, Complement, Irrelevant\n",
    "num_classes = 4 \n",
    "\n",
    "# initialize the model, loss, and optimizer\n",
    "model = MLP(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# for the training hyperparameter configuration \n",
    "# set the 4 epochs and Adam optimizer with values \n",
    "# epsilon (1e-8), learning rate (5e-5) and weight decay (0.01)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, eps=1e-8, weight_decay=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the data loader (train/test loader) to pass through the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings_df, labels_df, embedding_columns):\n",
    "        self.embeddings_df = embeddings_df\n",
    "        self.labels_df = labels_df\n",
    "        self.embedding_columns = embedding_columns\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the embedding row as a numpy array\n",
    "        embedding = self.embeddings_df.loc[idx, self.embedding_columns].values\n",
    "        \n",
    "        # Get the corresponding label\n",
    "        label = self.labels_df.loc[idx, 'encoded_labels']\n",
    "        \n",
    "        # Convert embedding to torch tensor\n",
    "        embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return embedding_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping the data\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through the result DataFrame which contains embeddings\n",
    "for i, row in result.iterrows():\n",
    "    # Get the embedding from the current row\n",
    "    embedding = row[embedding_columns].values  \n",
    "    embeddings.append(embedding)\n",
    "    \n",
    "    # Get the label by using the 'index' column instead of idx\n",
    "    index_value = row['index']  # Access the value in the 'index' column\n",
    "    label = task_2_train_sample.loc[task_2_train_sample['index'] == index_value, 'encoded_labels'].values[0]\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert lists to tensors\n",
    "embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset instances for train and test sets\n",
    "train_dataset = EmbeddingDataset(result, task_2_train, embedding_columns)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "# set the 4 epochs as defined in the paper \n",
    "def train_model(train_loader, model, criterion, optimizer, epochs=4):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation and output the f1 score \n",
    "def evaluate_model(test_loader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    # evaluate on the f1 score with micro averages\n",
    "    return f1_score(all_labels, all_preds, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting preliminalry results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_model(train_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "f1 = evaluate_model(test_loader, model)\n",
    "print(f'Micro F1 Score: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
