{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_path = os.path.join('.', 'esci-shopping-queries/data', 'shopping_queries_dataset_examples.parquet')\n",
    "products_path = os.path.join('.', 'esci-shopping-queries/data', 'shopping_queries_dataset_products.parquet')\n",
    "sources_path = os.path.join('.', 'esci-shopping-queries/data', 'shopping_queries_dataset_sources.csv')\n",
    "\n",
    "examples = dd.read_parquet(examples_path)\n",
    "products = dd.read_parquet(products_path)\n",
    "sources = dd.read_csv(sources_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_products = dd.merge(\n",
    "    examples,\n",
    "    products,\n",
    "    how='left',\n",
    "    left_on=['product_locale','product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "examples_products = examples_products[examples_products['product_locale'] == 'us']\n",
    "\n",
    "task_2 = examples_products[examples_products['large_version'] == 1]\n",
    "\n",
    "# another thing that I changed \n",
    "# encoding the esci labels \n",
    "label_mapping = {'E': 0, \n",
    "                 'S': 1, \n",
    "                 'C': 2, \n",
    "                 'I': 3}\n",
    "\n",
    "task_2['encoded_labels'] = task_2['esci_label'].map(label_mapping).astype(int)\n",
    "\n",
    "\n",
    "task_2_train = task_2[task_2['split'] == 'train']\n",
    "task_2_test = task_2[task_2['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carol\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "model = AutoModel.from_pretrained('distilroberta-base').to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    batch_size = 16  # Adjust this size\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # adjusting this for max pooling \n",
    "        batch_embeddings, _ = torch.max(outputs.last_hidden_state, dim=1)\n",
    "        batch_embeddings = batch_embeddings.cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def process_partition(partition):\n",
    "    query_embeddings = generate_embeddings(partition['query'])\n",
    "    product_title_embeddings = generate_embeddings(partition['product_title'])\n",
    "\n",
    "    combined = torch.cat((torch.tensor(query_embeddings), torch.tensor(product_title_embeddings)), dim=1).numpy()\n",
    "    \n",
    "    print(f'Combined shape: {combined.shape}')  # Expecting (n, 1536)\n",
    "\n",
    "    result = pd.DataFrame(combined, index=partition.index, columns=[f'embedding_{i}' for i in range(combined.shape[1])])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame and enerate column names\n",
    "meta = pd.DataFrame(columns=[f'embedding_{i}' for i in range(2 * 768)], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the total number of rows in the df\n",
    "total_rows = task_2_train.shape[0].compute()\n",
    "\n",
    "# calculates the fraction of rows needed to sample 10000 \n",
    "sample_fraction = 10000 / total_rows\n",
    "\n",
    "# samples a fraction of the df \n",
    "task_2_train_sample = task_2_train.sample(frac=sample_fraction, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicating the same as above but with the test data \n",
    "# computes the total number of rows in the df\n",
    "total_rows2 = task_2_test.shape[0].compute()\n",
    "\n",
    "# calculates the fraction of rows needed to sample 32719\n",
    "sample_fraction2 = 10000 / total_rows2\n",
    "\n",
    "# samples a fraction of the df \n",
    "task_2_test_sample = task_2_test.sample(frac=sample_fraction2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>small_version</th>\n",
       "      <th>large_version</th>\n",
       "      <th>split</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color</th>\n",
       "      <th>encoded_labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: sample, 17 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              example_id   query query_id product_id product_locale esci_label small_version large_version   split product_title product_description product_bullet_point product_brand product_color encoded_labels\n",
       "npartitions=1                                                                                                                                                                                                       \n",
       "                   int64  string    int64     string         string     string         int64         int64  string        string              string               string        string        string          int32\n",
       "                     ...     ...      ...        ...            ...        ...           ...           ...     ...           ...                 ...                  ...           ...           ...            ...\n",
       "Dask Name: sample, 17 graph layers"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_2_train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = task_2_train_sample.map_partitions(process_partition, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (10000, 1536)\n"
     ]
    }
   ],
   "source": [
    "result = result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (10000, 1536)\n"
     ]
    }
   ],
   "source": [
    "result2 = task_2_test_sample.map_partitions(process_partition, meta=meta)\n",
    "\n",
    "result2 = result2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_1526</th>\n",
       "      <th>embedding_1527</th>\n",
       "      <th>embedding_1528</th>\n",
       "      <th>embedding_1529</th>\n",
       "      <th>embedding_1530</th>\n",
       "      <th>embedding_1531</th>\n",
       "      <th>embedding_1532</th>\n",
       "      <th>embedding_1533</th>\n",
       "      <th>embedding_1534</th>\n",
       "      <th>embedding_1535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1322108</th>\n",
       "      <td>0.108705</td>\n",
       "      <td>0.234878</td>\n",
       "      <td>0.139451</td>\n",
       "      <td>0.315206</td>\n",
       "      <td>0.575189</td>\n",
       "      <td>-0.020448</td>\n",
       "      <td>0.148328</td>\n",
       "      <td>0.270037</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.128139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182917</td>\n",
       "      <td>0.098517</td>\n",
       "      <td>0.072425</td>\n",
       "      <td>0.049298</td>\n",
       "      <td>0.214076</td>\n",
       "      <td>0.081951</td>\n",
       "      <td>0.728155</td>\n",
       "      <td>0.395789</td>\n",
       "      <td>0.097055</td>\n",
       "      <td>0.106109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686437</th>\n",
       "      <td>0.128653</td>\n",
       "      <td>0.085391</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.089813</td>\n",
       "      <td>0.085738</td>\n",
       "      <td>-0.105572</td>\n",
       "      <td>0.078860</td>\n",
       "      <td>0.257313</td>\n",
       "      <td>0.048456</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269260</td>\n",
       "      <td>0.137719</td>\n",
       "      <td>0.355748</td>\n",
       "      <td>0.081922</td>\n",
       "      <td>0.274871</td>\n",
       "      <td>0.236418</td>\n",
       "      <td>0.301538</td>\n",
       "      <td>0.465827</td>\n",
       "      <td>0.222930</td>\n",
       "      <td>0.247738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135583</th>\n",
       "      <td>0.111750</td>\n",
       "      <td>0.206983</td>\n",
       "      <td>0.077644</td>\n",
       "      <td>-0.041382</td>\n",
       "      <td>0.817182</td>\n",
       "      <td>0.118343</td>\n",
       "      <td>0.085018</td>\n",
       "      <td>0.270429</td>\n",
       "      <td>0.149541</td>\n",
       "      <td>0.193659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307988</td>\n",
       "      <td>0.080208</td>\n",
       "      <td>0.200824</td>\n",
       "      <td>0.147451</td>\n",
       "      <td>0.306865</td>\n",
       "      <td>0.176306</td>\n",
       "      <td>0.818602</td>\n",
       "      <td>0.228154</td>\n",
       "      <td>0.178096</td>\n",
       "      <td>0.188407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566068</th>\n",
       "      <td>0.042544</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.084752</td>\n",
       "      <td>1.112144</td>\n",
       "      <td>-0.033281</td>\n",
       "      <td>0.045591</td>\n",
       "      <td>0.149777</td>\n",
       "      <td>0.165219</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365519</td>\n",
       "      <td>0.049413</td>\n",
       "      <td>0.259212</td>\n",
       "      <td>0.177284</td>\n",
       "      <td>0.178866</td>\n",
       "      <td>0.166594</td>\n",
       "      <td>0.643010</td>\n",
       "      <td>0.622623</td>\n",
       "      <td>0.218156</td>\n",
       "      <td>0.154736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075274</th>\n",
       "      <td>0.111251</td>\n",
       "      <td>0.145150</td>\n",
       "      <td>0.067874</td>\n",
       "      <td>0.081204</td>\n",
       "      <td>0.890214</td>\n",
       "      <td>-0.038702</td>\n",
       "      <td>0.101804</td>\n",
       "      <td>0.204691</td>\n",
       "      <td>0.066338</td>\n",
       "      <td>-0.024227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086346</td>\n",
       "      <td>0.180397</td>\n",
       "      <td>0.096318</td>\n",
       "      <td>0.155746</td>\n",
       "      <td>0.262609</td>\n",
       "      <td>0.157427</td>\n",
       "      <td>0.296502</td>\n",
       "      <td>0.315046</td>\n",
       "      <td>0.140594</td>\n",
       "      <td>0.244536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656352</th>\n",
       "      <td>0.238125</td>\n",
       "      <td>0.185477</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>0.261465</td>\n",
       "      <td>0.728569</td>\n",
       "      <td>-0.099423</td>\n",
       "      <td>0.199211</td>\n",
       "      <td>0.144787</td>\n",
       "      <td>0.068052</td>\n",
       "      <td>0.287547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617471</td>\n",
       "      <td>0.175846</td>\n",
       "      <td>0.573132</td>\n",
       "      <td>0.403009</td>\n",
       "      <td>0.477425</td>\n",
       "      <td>0.282736</td>\n",
       "      <td>0.492554</td>\n",
       "      <td>0.791985</td>\n",
       "      <td>0.324376</td>\n",
       "      <td>0.293089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603069</th>\n",
       "      <td>0.063194</td>\n",
       "      <td>0.217883</td>\n",
       "      <td>0.141870</td>\n",
       "      <td>0.170465</td>\n",
       "      <td>0.655924</td>\n",
       "      <td>-0.002001</td>\n",
       "      <td>0.123496</td>\n",
       "      <td>0.135748</td>\n",
       "      <td>0.042142</td>\n",
       "      <td>0.233673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195427</td>\n",
       "      <td>0.117208</td>\n",
       "      <td>0.421813</td>\n",
       "      <td>0.292528</td>\n",
       "      <td>0.437827</td>\n",
       "      <td>0.301911</td>\n",
       "      <td>0.611398</td>\n",
       "      <td>0.698750</td>\n",
       "      <td>0.273276</td>\n",
       "      <td>0.311094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815523</th>\n",
       "      <td>0.202807</td>\n",
       "      <td>0.227380</td>\n",
       "      <td>0.123965</td>\n",
       "      <td>0.099034</td>\n",
       "      <td>1.005392</td>\n",
       "      <td>0.480051</td>\n",
       "      <td>0.136926</td>\n",
       "      <td>0.050289</td>\n",
       "      <td>0.104759</td>\n",
       "      <td>0.291602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215847</td>\n",
       "      <td>0.118625</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.085508</td>\n",
       "      <td>0.313142</td>\n",
       "      <td>0.163584</td>\n",
       "      <td>0.459325</td>\n",
       "      <td>0.330234</td>\n",
       "      <td>0.162432</td>\n",
       "      <td>0.273381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732044</th>\n",
       "      <td>0.020295</td>\n",
       "      <td>0.086303</td>\n",
       "      <td>0.073896</td>\n",
       "      <td>0.338815</td>\n",
       "      <td>0.428603</td>\n",
       "      <td>0.239288</td>\n",
       "      <td>-0.016330</td>\n",
       "      <td>0.142109</td>\n",
       "      <td>0.039531</td>\n",
       "      <td>0.023234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202253</td>\n",
       "      <td>0.121261</td>\n",
       "      <td>0.296222</td>\n",
       "      <td>0.341769</td>\n",
       "      <td>0.307535</td>\n",
       "      <td>0.098422</td>\n",
       "      <td>0.500768</td>\n",
       "      <td>0.408014</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.286708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627562</th>\n",
       "      <td>0.106181</td>\n",
       "      <td>0.233717</td>\n",
       "      <td>0.106551</td>\n",
       "      <td>0.098796</td>\n",
       "      <td>0.503653</td>\n",
       "      <td>-0.022495</td>\n",
       "      <td>0.112938</td>\n",
       "      <td>0.054043</td>\n",
       "      <td>0.070425</td>\n",
       "      <td>0.099698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258592</td>\n",
       "      <td>0.237036</td>\n",
       "      <td>0.398716</td>\n",
       "      <td>0.132693</td>\n",
       "      <td>0.486761</td>\n",
       "      <td>0.192465</td>\n",
       "      <td>0.377412</td>\n",
       "      <td>0.606697</td>\n",
       "      <td>0.335490</td>\n",
       "      <td>0.408242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 1536 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "1322108     0.108705     0.234878     0.139451     0.315206     0.575189   \n",
       "686437      0.128653     0.085391     0.074830     0.089813     0.085738   \n",
       "2135583     0.111750     0.206983     0.077644    -0.041382     0.817182   \n",
       "1566068     0.042544     0.328947     0.044401     0.084752     1.112144   \n",
       "2075274     0.111251     0.145150     0.067874     0.081204     0.890214   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "1656352     0.238125     0.185477     0.054519     0.261465     0.728569   \n",
       "603069      0.063194     0.217883     0.141870     0.170465     0.655924   \n",
       "1815523     0.202807     0.227380     0.123965     0.099034     1.005392   \n",
       "1732044     0.020295     0.086303     0.073896     0.338815     0.428603   \n",
       "627562      0.106181     0.233717     0.106551     0.098796     0.503653   \n",
       "\n",
       "         embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "1322108    -0.020448     0.148328     0.270037     0.075255     0.128139  ...   \n",
       "686437     -0.105572     0.078860     0.257313     0.048456    -0.021896  ...   \n",
       "2135583     0.118343     0.085018     0.270429     0.149541     0.193659  ...   \n",
       "1566068    -0.033281     0.045591     0.149777     0.165219     0.013646  ...   \n",
       "2075274    -0.038702     0.101804     0.204691     0.066338    -0.024227  ...   \n",
       "...              ...          ...          ...          ...          ...  ...   \n",
       "1656352    -0.099423     0.199211     0.144787     0.068052     0.287547  ...   \n",
       "603069     -0.002001     0.123496     0.135748     0.042142     0.233673  ...   \n",
       "1815523     0.480051     0.136926     0.050289     0.104759     0.291602  ...   \n",
       "1732044     0.239288    -0.016330     0.142109     0.039531     0.023234  ...   \n",
       "627562     -0.022495     0.112938     0.054043     0.070425     0.099698  ...   \n",
       "\n",
       "         embedding_1526  embedding_1527  embedding_1528  embedding_1529  \\\n",
       "1322108        0.182917        0.098517        0.072425        0.049298   \n",
       "686437         0.269260        0.137719        0.355748        0.081922   \n",
       "2135583        0.307988        0.080208        0.200824        0.147451   \n",
       "1566068        0.365519        0.049413        0.259212        0.177284   \n",
       "2075274        0.086346        0.180397        0.096318        0.155746   \n",
       "...                 ...             ...             ...             ...   \n",
       "1656352        0.617471        0.175846        0.573132        0.403009   \n",
       "603069         0.195427        0.117208        0.421813        0.292528   \n",
       "1815523        0.215847        0.118625        0.004812        0.085508   \n",
       "1732044        0.202253        0.121261        0.296222        0.341769   \n",
       "627562         0.258592        0.237036        0.398716        0.132693   \n",
       "\n",
       "         embedding_1530  embedding_1531  embedding_1532  embedding_1533  \\\n",
       "1322108        0.214076        0.081951        0.728155        0.395789   \n",
       "686437         0.274871        0.236418        0.301538        0.465827   \n",
       "2135583        0.306865        0.176306        0.818602        0.228154   \n",
       "1566068        0.178866        0.166594        0.643010        0.622623   \n",
       "2075274        0.262609        0.157427        0.296502        0.315046   \n",
       "...                 ...             ...             ...             ...   \n",
       "1656352        0.477425        0.282736        0.492554        0.791985   \n",
       "603069         0.437827        0.301911        0.611398        0.698750   \n",
       "1815523        0.313142        0.163584        0.459325        0.330234   \n",
       "1732044        0.307535        0.098422        0.500768        0.408014   \n",
       "627562         0.486761        0.192465        0.377412        0.606697   \n",
       "\n",
       "         embedding_1534  embedding_1535  \n",
       "1322108        0.097055        0.106109  \n",
       "686437         0.222930        0.247738  \n",
       "2135583        0.178096        0.188407  \n",
       "1566068        0.218156        0.154736  \n",
       "2075274        0.140594        0.244536  \n",
       "...                 ...             ...  \n",
       "1656352        0.324376        0.293089  \n",
       "603069         0.273276        0.311094  \n",
       "1815523        0.162432        0.273381  \n",
       "1732044        0.281700        0.286708  \n",
       "627562         0.335490        0.408242  \n",
       "\n",
       "[10000 rows x 1536 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a subset that only has the indicies and the coresponding label \n",
    "task_2_train = task_2_train.compute()\n",
    "task_2_test = task_2_test.compute()\n",
    "\n",
    "type(task_2_train)\n",
    "type(task_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Multi-Layer Preceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the Multi-Layer Preceptron model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  \n",
    "            nn.Dropout(0.1),  \n",
    "            nn.Linear(hidden_size, num_classes)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputting the parameters\n",
    "\n",
    "# the size of the concatenated embeddings(768 + 768)\n",
    "input_size = 1536  \n",
    "hidden_size = 128\n",
    "# number of classes Exact, Substitute, Complement, Irrelevant\n",
    "num_classes = 4 \n",
    "\n",
    "# initialize the model, loss, and optimizer\n",
    "model = MLP(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# for the training hyperparameter configuration \n",
    "# set the 4 epochs and Adam optimizer with values \n",
    "# epsilon (1e-8), learning rate (5e-5) and weight decay (0.01)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, eps=1e-8, weight_decay=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the data loader (train/test loader) to pass through the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the data loader with the accurate indexes \n",
    "\n",
    "# making sure the indicies are in the same data format \n",
    "subset_indices = result.index\n",
    "subset_indices = subset_indices.astype(int)\n",
    "task_2_train_indices = task_2_train.index.astype(int)\n",
    "\n",
    "# getting the indexes that were used in the sample group of embeddings\n",
    "# maing sure they are also in the og training set \n",
    "valid_indices = task_2_train_indices[task_2_train_indices.isin(subset_indices)]\n",
    "\n",
    "# making subset labels which filters the train dataset to get the labels the correspond to the embeddings \n",
    "subset_labels = task_2_train.loc[valid_indices, 'encoded_labels'] \n",
    "# make it into a dataframe\n",
    "subset_labels = subset_labels.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the same with the test \n",
    "# creating the data loader with the accurate indexes \n",
    "\n",
    "# making sure the indicies are in the same data format \n",
    "subset_indices2 = result2.index\n",
    "subset_indices2 = subset_indices2.astype(int)\n",
    "task_2_test_indices = task_2_test.index.astype(int)\n",
    "\n",
    "# getting the indexes that were used in the sample group of embeddings\n",
    "# maing sure they are also in the og training set \n",
    "valid_indices2 = task_2_test_indices[task_2_test_indices.isin(subset_indices2)]\n",
    "\n",
    "# making subset labels which filters the train dataset to get the labels the correspond to the embeddings \n",
    "subset_labels2 = task_2_test.loc[valid_indices2, 'encoded_labels'] \n",
    "# make it into a dataframe\n",
    "subset_labels2 = subset_labels2.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result2.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         encoded_labels\n",
      "196                   0\n",
      "287                   0\n",
      "795                   0\n",
      "835                   1\n",
      "1448                  3\n",
      "...                 ...\n",
      "2260438               2\n",
      "2260686               3\n",
      "2261045               0\n",
      "2261125               0\n",
      "2567053               3\n",
      "\n",
      "[10000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(subset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         encoded_labels\n",
      "410                   0\n",
      "429                   3\n",
      "560                   0\n",
      "580                   0\n",
      "581                   1\n",
      "...                 ...\n",
      "2258965               0\n",
      "2259219               0\n",
      "2260170               0\n",
      "2260571               0\n",
      "2533306               0\n",
      "\n",
      "[10000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(subset_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(subset_labels['encoded_labels'].iloc[196])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(subset_labels2['encoded_labels'].iloc[410])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(subset_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (10000, 1536)\n"
     ]
    }
   ],
   "source": [
    "class ESCIDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings.values\n",
    "        # this should be (size, 1563)\n",
    "        print(\"Shape of embeddings:\", self.embeddings.shape)\n",
    "        self.labels = labels   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# create DataLoader\n",
    "train_dataset = ESCIDataset(embeddings=result, labels=subset_labels['encoded_labels'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # adjust the batch size as needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (10000, 1536)\n"
     ]
    }
   ],
   "source": [
    "# making the test loader \n",
    "test_dataset = ESCIDataset(embeddings=result2, labels=subset_labels2['encoded_labels'].values)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of embeddings: 10000\n",
      "Length of labels: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of embeddings:\", len(train_dataset.embeddings))\n",
    "print(\"Length of labels:\", len(train_dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of embeddings: <class 'numpy.ndarray'>\n",
      "Type of labels: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# these should both be 'numpy.ndarray' or theres a problem \n",
    "print(\"Type of embeddings:\", type(train_dataset.embeddings))\n",
    "print(\"Type of labels:\", type(train_dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 - Embedding: [0.17990594 0.10993975 0.12278097 ... 0.43050417 0.19597918 0.21272719], Label: 0\n",
      "Sample 1 - Embedding: [0.10828137 0.07075226 0.07835397 ... 0.52852386 0.23825216 0.30547935], Label: 0\n",
      "Sample 2 - Embedding: [0.21993501 0.3172615  0.04313886 ... 0.7421779  0.23599522 0.31108263], Label: 0\n",
      "Sample 3 - Embedding: [0.10928485 0.3230517  0.13458526 ... 0.43360886 0.22882652 0.26526317], Label: 1\n",
      "Sample 4 - Embedding: [0.11612305 0.11765453 0.06042268 ... 0.5264007  0.28494015 0.25032914], Label: 3\n"
     ]
    }
   ],
   "source": [
    "# look at the samples to double check everythign is looking right \n",
    "for i in range(5):  \n",
    "    embedding, label = train_dataset[i]\n",
    "    print(f\"sample {i} - embedding: {embedding}, label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([32, 1536])\n",
      "Labels shape: torch.Size([32])\n",
      "Batch shape: torch.Size([16, 1536])\n",
      "Labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# making sure the batch sizes look correct \n",
    "for embeddings, labels in train_loader:\n",
    "    print(f\"Batch shape: {embeddings.shape}\")  \n",
    "    print(f\"Labels shape: {labels.shape}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.8834\n",
      "Epoch 2/4, Loss: 0.8547\n",
      "Epoch 3/4, Loss: 0.8471\n",
      "Epoch 4/4, Loss: 0.8434\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# set the 4 epochs as defined in the paper \n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=4):\n",
    "    model.train()  # set model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (embeddings, labels) in enumerate(train_loader):\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(embeddings.float())  # Forward pass\n",
    "            # converting the labels to long in order to \n",
    "            labels = labels.long()\n",
    "            # calculate the loss \n",
    "            loss = criterion(outputs, labels) \n",
    "            # backpropogation \n",
    "            loss.backward() \n",
    "            # updating the weights \n",
    "            optimizer.step()  \n",
    "\n",
    "            # add up the loss \n",
    "            epoch_loss += loss.item()  \n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# run the training model with the 10000 samples \n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation and output the f1 score \n",
    "def evaluate_model(test_loader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            \n",
    "    # evaluate on the f1 score with micro averages\n",
    "    return f1_score(all_labels, all_preds, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting preliminalry results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.6519\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "f1 = evaluate_model(test_loader, model)\n",
    "print(f'Micro F1 Score: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
