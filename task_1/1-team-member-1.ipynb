{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahlawlis/Desktop/repos/esci-shopping-queries/env/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/sarahlawlis/Desktop/repos/esci-shopping-queries/env/lib/python3.11/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_path = os.path.join('..', 'data', 'shopping_queries_dataset_examples.parquet')\n",
    "products_path = os.path.join('..', 'data', 'shopping_queries_dataset_products.parquet')\n",
    "sources_path = os.path.join('..', 'data', 'shopping_queries_dataset_sources.csv')\n",
    "\n",
    "examples = dd.read_parquet(examples_path)\n",
    "products = dd.read_parquet(products_path)\n",
    "sources = dd.read_csv(sources_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_products = dd.merge(\n",
    "    examples,\n",
    "    products,\n",
    "    how='left',\n",
    "    left_on=['product_locale','product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "examples_products = examples_products[examples_products['product_locale'] == 'us']\n",
    "\n",
    "task_2 = examples_products[examples_products['large_version'] == 1]\n",
    "\n",
    "label_mapping = {'E': 0, \n",
    "                 'S': 1, \n",
    "                 'C': 2, \n",
    "                 'I': 3}\n",
    "\n",
    "task_2['encoded_labels'] = task_2['esci_label'].map(label_mapping).astype(int)\n",
    "\n",
    "task_2_train = task_2[task_2['split'] == 'train']\n",
    "task_2_test = task_2[task_2['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahlawlis/Desktop/repos/esci-shopping-queries/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    batch_size = 128  # Adjust this size\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def process_partition(partition):\n",
    "    query_embeddings = generate_embeddings(partition['query'])\n",
    "    product_title_embeddings = generate_embeddings(partition['product_title'])\n",
    "\n",
    "    combined = torch.cat((torch.tensor(query_embeddings), torch.tensor(product_title_embeddings)), dim=1).numpy()\n",
    "    \n",
    "    print(f'Combined shape: {combined.shape}')  # expecting (n, 1536)\n",
    "\n",
    "    result = pd.DataFrame(combined, index=partition.index, columns=[f'embedding_{i}' for i in range(combined.shape[1])])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.DataFrame(columns=[f'embedding_{i}' for i in range(2 * 768)], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = task_2_train.shape[0].compute()\n",
    "\n",
    "sample_fraction = 10000 / total_rows\n",
    "\n",
    "task_2_train_sample = task_2_train.sample(frac=sample_fraction, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = task_2_train.map_partitions(process_partition, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (10000, 1536)\n",
      "Combined shape: (10000, 1536)\n"
     ]
    }
   ],
   "source": [
    "result = result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows2 = task_2_test.shape[0].compute()\n",
    "\n",
    "sample_fraction2 = 10000 / total_rows2\n",
    "\n",
    "task_2_test_sample = task_2_test.sample(frac=sample_fraction2, random_state=21)\n",
    "\n",
    "# query_texts = task_2_test_sample['query'].tolist()\n",
    "# product_titles = task_2_test_sample['product_title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = task_2_test.map_partitions(process_partition, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_1526</th>\n",
       "      <th>embedding_1527</th>\n",
       "      <th>embedding_1528</th>\n",
       "      <th>embedding_1529</th>\n",
       "      <th>embedding_1530</th>\n",
       "      <th>embedding_1531</th>\n",
       "      <th>embedding_1532</th>\n",
       "      <th>embedding_1533</th>\n",
       "      <th>embedding_1534</th>\n",
       "      <th>embedding_1535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1135254</th>\n",
       "      <td>-0.290532</td>\n",
       "      <td>-0.063103</td>\n",
       "      <td>0.026229</td>\n",
       "      <td>-0.118062</td>\n",
       "      <td>-0.053797</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.179665</td>\n",
       "      <td>0.128031</td>\n",
       "      <td>-0.258895</td>\n",
       "      <td>-0.140017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108044</td>\n",
       "      <td>-0.016486</td>\n",
       "      <td>-0.091957</td>\n",
       "      <td>-0.185485</td>\n",
       "      <td>0.208071</td>\n",
       "      <td>-0.012939</td>\n",
       "      <td>-0.199160</td>\n",
       "      <td>0.007739</td>\n",
       "      <td>0.308227</td>\n",
       "      <td>0.313024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507395</th>\n",
       "      <td>-0.285455</td>\n",
       "      <td>-0.111262</td>\n",
       "      <td>-0.113142</td>\n",
       "      <td>-0.158004</td>\n",
       "      <td>-0.117075</td>\n",
       "      <td>0.013010</td>\n",
       "      <td>0.261024</td>\n",
       "      <td>0.233638</td>\n",
       "      <td>-0.136493</td>\n",
       "      <td>-0.161162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081286</td>\n",
       "      <td>-0.443307</td>\n",
       "      <td>-0.073551</td>\n",
       "      <td>-0.483737</td>\n",
       "      <td>0.111754</td>\n",
       "      <td>-0.181978</td>\n",
       "      <td>0.077074</td>\n",
       "      <td>-0.073647</td>\n",
       "      <td>0.051551</td>\n",
       "      <td>0.116166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421122</th>\n",
       "      <td>-0.178601</td>\n",
       "      <td>0.053528</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.139839</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>-0.052566</td>\n",
       "      <td>0.326615</td>\n",
       "      <td>0.228888</td>\n",
       "      <td>-0.437036</td>\n",
       "      <td>-0.257336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257401</td>\n",
       "      <td>-0.436189</td>\n",
       "      <td>-0.060040</td>\n",
       "      <td>-0.092231</td>\n",
       "      <td>0.288025</td>\n",
       "      <td>-0.158821</td>\n",
       "      <td>-0.056691</td>\n",
       "      <td>-0.083708</td>\n",
       "      <td>-0.046926</td>\n",
       "      <td>0.269202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977862</th>\n",
       "      <td>-0.152321</td>\n",
       "      <td>-0.181627</td>\n",
       "      <td>-0.028486</td>\n",
       "      <td>-0.103172</td>\n",
       "      <td>-0.212782</td>\n",
       "      <td>-0.040475</td>\n",
       "      <td>0.177514</td>\n",
       "      <td>0.349715</td>\n",
       "      <td>-0.080296</td>\n",
       "      <td>-0.143331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073216</td>\n",
       "      <td>-0.087939</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>-0.086273</td>\n",
       "      <td>0.165844</td>\n",
       "      <td>-0.034154</td>\n",
       "      <td>-0.117882</td>\n",
       "      <td>0.007349</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.228783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186808</th>\n",
       "      <td>-0.246065</td>\n",
       "      <td>-0.205255</td>\n",
       "      <td>-0.052422</td>\n",
       "      <td>-0.045568</td>\n",
       "      <td>0.063240</td>\n",
       "      <td>-0.169057</td>\n",
       "      <td>0.067012</td>\n",
       "      <td>0.268431</td>\n",
       "      <td>-0.144231</td>\n",
       "      <td>-0.409057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308713</td>\n",
       "      <td>-0.443466</td>\n",
       "      <td>0.051421</td>\n",
       "      <td>-0.294180</td>\n",
       "      <td>0.144461</td>\n",
       "      <td>0.061179</td>\n",
       "      <td>-0.215118</td>\n",
       "      <td>-0.250605</td>\n",
       "      <td>-0.057553</td>\n",
       "      <td>0.176653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904339</th>\n",
       "      <td>-0.111236</td>\n",
       "      <td>-0.239602</td>\n",
       "      <td>-0.068928</td>\n",
       "      <td>-0.018820</td>\n",
       "      <td>-0.234648</td>\n",
       "      <td>0.094480</td>\n",
       "      <td>0.295593</td>\n",
       "      <td>0.181494</td>\n",
       "      <td>-0.154788</td>\n",
       "      <td>-0.032624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140249</td>\n",
       "      <td>-0.542634</td>\n",
       "      <td>0.024627</td>\n",
       "      <td>-0.191631</td>\n",
       "      <td>0.156213</td>\n",
       "      <td>-0.042998</td>\n",
       "      <td>-0.054532</td>\n",
       "      <td>-0.069859</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.149266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973076</th>\n",
       "      <td>-0.280886</td>\n",
       "      <td>-0.076505</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>-0.162118</td>\n",
       "      <td>-0.040654</td>\n",
       "      <td>-0.014583</td>\n",
       "      <td>0.137229</td>\n",
       "      <td>0.208141</td>\n",
       "      <td>-0.280146</td>\n",
       "      <td>-0.179859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007630</td>\n",
       "      <td>-0.461214</td>\n",
       "      <td>-0.157896</td>\n",
       "      <td>-0.087187</td>\n",
       "      <td>0.232041</td>\n",
       "      <td>-0.056665</td>\n",
       "      <td>-0.133207</td>\n",
       "      <td>-0.189570</td>\n",
       "      <td>0.073114</td>\n",
       "      <td>0.221584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656276</th>\n",
       "      <td>-0.221028</td>\n",
       "      <td>0.025194</td>\n",
       "      <td>-0.078133</td>\n",
       "      <td>-0.123258</td>\n",
       "      <td>-0.090522</td>\n",
       "      <td>-0.171234</td>\n",
       "      <td>0.167856</td>\n",
       "      <td>0.217071</td>\n",
       "      <td>-0.198434</td>\n",
       "      <td>0.038669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093497</td>\n",
       "      <td>-0.406056</td>\n",
       "      <td>0.004769</td>\n",
       "      <td>-0.291771</td>\n",
       "      <td>0.251960</td>\n",
       "      <td>-0.204405</td>\n",
       "      <td>-0.226528</td>\n",
       "      <td>-0.396483</td>\n",
       "      <td>0.053727</td>\n",
       "      <td>0.434428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897766</th>\n",
       "      <td>-0.107969</td>\n",
       "      <td>-0.252370</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>-0.037156</td>\n",
       "      <td>-0.121543</td>\n",
       "      <td>-0.076469</td>\n",
       "      <td>0.267313</td>\n",
       "      <td>0.412872</td>\n",
       "      <td>-0.231740</td>\n",
       "      <td>-0.202620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180342</td>\n",
       "      <td>-0.311242</td>\n",
       "      <td>-0.029214</td>\n",
       "      <td>-0.283201</td>\n",
       "      <td>0.087618</td>\n",
       "      <td>0.084257</td>\n",
       "      <td>-0.113983</td>\n",
       "      <td>-0.089989</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.227009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058305</th>\n",
       "      <td>-0.158857</td>\n",
       "      <td>-0.079989</td>\n",
       "      <td>-0.173021</td>\n",
       "      <td>-0.191162</td>\n",
       "      <td>0.131657</td>\n",
       "      <td>-0.080079</td>\n",
       "      <td>-0.020804</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>-0.398208</td>\n",
       "      <td>-0.213757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286847</td>\n",
       "      <td>-0.195048</td>\n",
       "      <td>0.183314</td>\n",
       "      <td>-0.189234</td>\n",
       "      <td>0.029039</td>\n",
       "      <td>-0.400878</td>\n",
       "      <td>-0.140198</td>\n",
       "      <td>-0.148307</td>\n",
       "      <td>-0.008428</td>\n",
       "      <td>0.148791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 1536 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "1135254    -0.290532    -0.063103     0.026229    -0.118062    -0.053797   \n",
       "507395     -0.285455    -0.111262    -0.113142    -0.158004    -0.117075   \n",
       "421122     -0.178601     0.053528    -0.211663    -0.139839     0.103700   \n",
       "977862     -0.152321    -0.181627    -0.028486    -0.103172    -0.212782   \n",
       "2186808    -0.246065    -0.205255    -0.052422    -0.045568     0.063240   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "904339     -0.111236    -0.239602    -0.068928    -0.018820    -0.234648   \n",
       "973076     -0.280886    -0.076505     0.004162    -0.162118    -0.040654   \n",
       "1656276    -0.221028     0.025194    -0.078133    -0.123258    -0.090522   \n",
       "897766     -0.107969    -0.252370     0.010800    -0.037156    -0.121543   \n",
       "2058305    -0.158857    -0.079989    -0.173021    -0.191162     0.131657   \n",
       "\n",
       "         embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "1135254     0.001423     0.179665     0.128031    -0.258895    -0.140017  ...   \n",
       "507395      0.013010     0.261024     0.233638    -0.136493    -0.161162  ...   \n",
       "421122     -0.052566     0.326615     0.228888    -0.437036    -0.257336  ...   \n",
       "977862     -0.040475     0.177514     0.349715    -0.080296    -0.143331  ...   \n",
       "2186808    -0.169057     0.067012     0.268431    -0.144231    -0.409057  ...   \n",
       "...              ...          ...          ...          ...          ...  ...   \n",
       "904339      0.094480     0.295593     0.181494    -0.154788    -0.032624  ...   \n",
       "973076     -0.014583     0.137229     0.208141    -0.280146    -0.179859  ...   \n",
       "1656276    -0.171234     0.167856     0.217071    -0.198434     0.038669  ...   \n",
       "897766     -0.076469     0.267313     0.412872    -0.231740    -0.202620  ...   \n",
       "2058305    -0.080079    -0.020804     0.326394    -0.398208    -0.213757  ...   \n",
       "\n",
       "         embedding_1526  embedding_1527  embedding_1528  embedding_1529  \\\n",
       "1135254        0.108044       -0.016486       -0.091957       -0.185485   \n",
       "507395         0.081286       -0.443307       -0.073551       -0.483737   \n",
       "421122         0.257401       -0.436189       -0.060040       -0.092231   \n",
       "977862         0.073216       -0.087939        0.008919       -0.086273   \n",
       "2186808        0.308713       -0.443466        0.051421       -0.294180   \n",
       "...                 ...             ...             ...             ...   \n",
       "904339         0.140249       -0.542634        0.024627       -0.191631   \n",
       "973076        -0.007630       -0.461214       -0.157896       -0.087187   \n",
       "1656276        0.093497       -0.406056        0.004769       -0.291771   \n",
       "897766         0.180342       -0.311242       -0.029214       -0.283201   \n",
       "2058305        0.286847       -0.195048        0.183314       -0.189234   \n",
       "\n",
       "         embedding_1530  embedding_1531  embedding_1532  embedding_1533  \\\n",
       "1135254        0.208071       -0.012939       -0.199160        0.007739   \n",
       "507395         0.111754       -0.181978        0.077074       -0.073647   \n",
       "421122         0.288025       -0.158821       -0.056691       -0.083708   \n",
       "977862         0.165844       -0.034154       -0.117882        0.007349   \n",
       "2186808        0.144461        0.061179       -0.215118       -0.250605   \n",
       "...                 ...             ...             ...             ...   \n",
       "904339         0.156213       -0.042998       -0.054532       -0.069859   \n",
       "973076         0.232041       -0.056665       -0.133207       -0.189570   \n",
       "1656276        0.251960       -0.204405       -0.226528       -0.396483   \n",
       "897766         0.087618        0.084257       -0.113983       -0.089989   \n",
       "2058305        0.029039       -0.400878       -0.140198       -0.148307   \n",
       "\n",
       "         embedding_1534  embedding_1535  \n",
       "1135254        0.308227        0.313024  \n",
       "507395         0.051551        0.116166  \n",
       "421122        -0.046926        0.269202  \n",
       "977862         0.295800        0.228783  \n",
       "2186808       -0.057553        0.176653  \n",
       "...                 ...             ...  \n",
       "904339         0.013905        0.149266  \n",
       "973076         0.073114        0.221584  \n",
       "1656276        0.053727        0.434428  \n",
       "897766         0.035200        0.227009  \n",
       "2058305       -0.008428        0.148791  \n",
       "\n",
       "[10000 rows x 1536 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_2_train = task_2_train.compute()\n",
    "task_2_test = task_2_test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        pooled_output_size = hidden_size // 2\n",
    "        self.fc2 = nn.Linear(pooled_output_size, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1536\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "\n",
    "model = FullyConnected(input_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, eps=1e-8, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = result.index\n",
    "subset_indices = subset_indices.astype(int)\n",
    "task_2_train_indicies = task_2_train.index.astype(int)\n",
    "\n",
    "valid_indicies = task_2_train_indicies[task_2_train_indicies.isin(subset_indices)]\n",
    "subset_labels = task_2_train.loc[valid_indicies, 'encoded_labels']\n",
    "subset_labels = subset_labels.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices2 = result2.index\n",
    "subset_indices2 = subset_indices2.astype(int)\n",
    "task_2_test_indices = task_2_test.index.astype(int)\n",
    " \n",
    "valid_indices2 = task_2_test_indices[task_2_test_indices.isin(subset_indices2)]\n",
    "subset_labels2 = task_2_test.loc[valid_indices2, 'encoded_labels'] \n",
    "subset_labels2 = subset_labels2.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.sort_index()\n",
    "result2 = result2.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESCIDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings.values\n",
    "\n",
    "        print(f'Shape of embeddings: {self.embeddings.shape}')\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (10000, 1536)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ESCIDataset(embeddings=result, labels=subset_labels['encoded_labels'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (10000, 1536)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ESCIDataset(embeddings=result2, labels=subset_labels2['encoded_labels'].values)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of embeddings: <class 'numpy.ndarray'>\n",
      "Type of labels: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of embeddings:\", type(train_dataset.embeddings))\n",
    "print(\"Type of labels:\", type(train_dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Loss: 0.9037\n",
      "Epoch 2/8, Loss: 0.8495\n",
      "Epoch 3/8, Loss: 0.8400\n",
      "Epoch 4/8, Loss: 0.8356\n",
      "Epoch 5/8, Loss: 0.8326\n",
      "Epoch 6/8, Loss: 0.8287\n",
      "Epoch 7/8, Loss: 0.8273\n",
      "Epoch 8/8, Loss: 0.8256\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=8):\n",
    "    model.train()  # set model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (embeddings, labels) in enumerate(train_loader):\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(embeddings.float())  # Forward pass\n",
    "            # converting the labels to long in order to \n",
    "            labels = labels.long()\n",
    "            # calculate the loss \n",
    "            loss = criterion(outputs, labels) \n",
    "            # backpropogation \n",
    "            loss.backward() \n",
    "            # updating the weights \n",
    "            optimizer.step()  \n",
    "\n",
    "            # add up the loss \n",
    "            epoch_loss += loss.item()  \n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# run the training model with the 10000 samples \n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_loader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            \n",
    "    # evaluate on the f1 score with micro averages\n",
    "    return f1_score(all_labels, all_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.6517\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluate_model(test_loader, model)\n",
    "print(f'Micro F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_capture_mismatches(test_loader, model, task_2_test):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # convert task_2_test to pandas df if it's a dask df\n",
    "    if hasattr(task_2_test, 'compute'):\n",
    "        test_df = task_2_test[['query', 'product_title', 'encoded_labels']].compute()\n",
    "    else:\n",
    "        test_df = task_2_test[['query', 'product_title', 'encoded_labels']]\n",
    "\n",
    "    test_df['predicted_label'] = all_preds\n",
    "    test_df['true_label'] = all_labels\n",
    "    \n",
    "    mismatch_df = test_df[test_df['true_label'] != test_df['predicted_label']]\n",
    "    \n",
    "    return mismatch_df\n",
    "\n",
    "mismatch_df = evaluate_and_capture_mismatches(test_loader, model, task_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 queries with the most mismatches:\n",
      " query\n",
      "pet boundry flags                           4\n",
      "lv coin purse                               3\n",
      "lemon essential oils for body butter        3\n",
      "gucci disco                                 3\n",
      "tecnu wipes                                 3\n",
      "die grinder set                             3\n",
      "macbook air                                 3\n",
      "toothpaste without sodium lauryl sulfate    3\n",
      "taupe blackout curtains                     3\n",
      "electric motor mongose                      3\n",
      "Name: count, dtype: int64[pyarrow]\n",
      "\n",
      "Top 10 most common words in mismatched entries:\n",
      " [('for', 1681), ('-', 1084), ('with', 981), ('and', 881), ('&', 412), ('of', 357), ('Black', 261), ('|', 258), ('2', 252), ('to', 218)]\n"
     ]
    }
   ],
   "source": [
    "# count top 10mismatches per query\n",
    "mismatch_counts_per_query = mismatch_df['query'].value_counts().head(10) \n",
    "mismatch_counts_per_product = mismatch_df['product_title'].value_counts().head(10)\n",
    "\n",
    "all_text = ' '.join(mismatch_df['query'].tolist() + mismatch_df['product_title'].tolist())\n",
    "word_counts = Counter(all_text.split()).most_common(10)  # Top 10 common words\n",
    "\n",
    "print(\"Top 10 queries with the most mismatches:\\n\", mismatch_counts_per_query)\n",
    "print(\"\\nTop 10 most common words in mismatched entries:\\n\", word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
