{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import autocast\n",
    "from transformers import RobertaTokenizer, RobertaForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoTokenizer, AdamWeightDecay, AutoModelForMaskedLM, default_data_collator, RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "import collections\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "# Update to Pandas to Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install tf-keras\n",
    "#!pip install --upgrade huggingface_hub\n",
    "#! pip install 'transformers[torch]' accelerate\n",
    "#! pip install torch torchvision torchaudio\n",
    "#! pip install pyspellchecker\n",
    "#! pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #2B5269; color: white; padding: 10px; \">\n",
    "<h1> Pre-processing <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9c3e302380467b925982cce3ea9dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0703cddda9a14e4ebfe9a177b8e8452c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e167ba1d48f41d49ea681bf837ffa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load Google Shopping dataset from hugging face\n",
    "# link to dataset: https://huggingface.co/datasets/Marqo/google-shopping-general-eval/viewer?sql=--+The+SQL+console+is+powered+by+DuckDB+WASM+and+runs+entirely+in+the+browser.%0A--+Get+started+by+typing+a+query+or+selecting+a+view+from+the+options+below.%0ASELECT+*+FROM+data+LIMIT+10%3B\n",
    "pretrain_dataset = load_dataset('Marqo/google-shopping-general-eval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_dataset = pretrain_dataset['data']\n",
    "pretrain_dataset = pretrain_dataset.remove_columns(['image', 'item_ID', 'position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': ['Adaptive Drinking Straws', \"Baby Boys' Outerwear Jackets\"],\n",
       " 'title': ['Swig Reusable Straws + Cleaning Brush (Jeepers Creepers + Black Glitter)',\n",
       "  \"Carter's Baby Boy's Hooded Sweater Jacket Size 12 Months Beige Fleece\"]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('automotive', 62180), ('s', 59374), ('replacement', 52626), ('children', 22013), ('books', 19676), ('women', 19647), ('sports', 17840), ('travel', 17836), ('accessories', 17691), ('kits', 16576)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Function to tokenize and clean text (remove punctuation, lowercasing, etc.)\n",
    "def tokenize(text):\n",
    "    # Convert to lowercase and remove non-alphabetic characters\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words\n",
    "\n",
    "# Apply tokenization and count frequencies\n",
    "word_counter = Counter()\n",
    "\n",
    "# Pull queries column to find misspellings\n",
    "queries = pretrain_dataset['query']\n",
    "\n",
    "for text in queries:\n",
    "    words = tokenize(text)\n",
    "    word_counter.update(words)\n",
    "\n",
    "# View the most common words\n",
    "print(word_counter.most_common(10))  # Adjust the number to see more/less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled Word: dns, Frequency: 11\n",
      "Misspelled Word: lifejackets, Frequency: 18\n",
      "Misspelled Word: jicama, Frequency: 19\n",
      "Misspelled Word: pashminas, Frequency: 19\n",
      "Misspelled Word: oled, Frequency: 20\n",
      "Misspelled Word: henley, Frequency: 21\n",
      "Misspelled Word: marsala, Frequency: 21\n",
      "Misspelled Word: paracord, Frequency: 22\n",
      "Misspelled Word: mortadella, Frequency: 23\n",
      "Misspelled Word: bodyboards, Frequency: 25\n",
      "Misspelled Word: eggnogs, Frequency: 28\n",
      "Misspelled Word: lisbon, Frequency: 28\n",
      "Misspelled Word: craniomandibular, Frequency: 29\n",
      "Misspelled Word: temporomandibular, Frequency: 29\n",
      "Misspelled Word: clawfoot, Frequency: 30\n",
      "Misspelled Word: skooters, Frequency: 32\n",
      "Misspelled Word: shilajit, Frequency: 32\n",
      "Misspelled Word: edamame, Frequency: 32\n",
      "Misspelled Word: venice, Frequency: 33\n",
      "Misspelled Word: kauai, Frequency: 33\n",
      "Misspelled Word: kettlebells, Frequency: 33\n",
      "Misspelled Word: quickbooks, Frequency: 33\n",
      "Misspelled Word: yellowstone, Frequency: 33\n",
      "Misspelled Word: darbukas, Frequency: 33\n",
      "Misspelled Word: dna, Frequency: 33\n",
      "Misspelled Word: daypacks, Frequency: 34\n",
      "Misspelled Word: bariatrics, Frequency: 34\n",
      "Misspelled Word: headwraps, Frequency: 34\n",
      "Misspelled Word: rhine, Frequency: 34\n",
      "Misspelled Word: varnishers, Frequency: 34\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# initialize spellchecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# loop through word frequencies to find words that might be misspelled\n",
    "misspelled_words = {word: freq for word, freq in word_counter.items() if word not in spell}\n",
    "\n",
    "# Sort misspelled words dict\n",
    "sorted_misspelled = sorted(misspelled_words.items(), key= lambda x: x[1])\n",
    "\n",
    "# Print misspelled words and their frequencies\n",
    "for word, freq in sorted_misspelled[:30]:  # Adjust the number as needed\n",
    "    print(f'Misspelled Word: {word}, Frequency: {freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine relevant text columns for training\n",
    "pretrain_dataset = pretrain_dataset.map(lambda example: {\n",
    "    'text': str(example['query']) + \" \" + str(example['title'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove special characters and lowercase the text\n",
    "puncts = string.punctuation\n",
    "def process_text(batch, puncts):\n",
    "    # Remove punctuation and lowercase each text in batch\n",
    "    texts = []\n",
    "    for text in batch['text']:\n",
    "        text = str(text)  # Ensure it's a string\n",
    "        for punc in puncts:\n",
    "            text = text.replace(punc, '')\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        texts.append(text)\n",
    "    return {'text': texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934ee69cf16644dc81caac2a80d342cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/982700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the text processing with batch=True and disable caching\n",
    "pretrain_dataset = pretrain_dataset.map(\n",
    "    lambda batch: process_text(batch, puncts), \n",
    "    batched=True, \n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'title', 'text'],\n",
       "    num_rows: 982700\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adaptive drinking straws swig reusable straws  cleaning brush jeepers creepers  black glitter', 'baby boys outerwear jackets carters baby boys hooded sweater jacket size 12 months beige fleece', 'mixed drinkware sets mismatched colors wine  water glasses collection set of 4 vintage', 'rod end bearings aurora mg8 rod end bearing 12', 'hanukkah music a hanukkah suite']\n"
     ]
    }
   ],
   "source": [
    "print(pretrain_dataset['text'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2B5269; color: white; padding: 10px; \">\n",
    "<h1> Training Distilled Roberta Model<h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasburns/opt/miniconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Distilled RoBerta tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "model = RobertaForCausalLM.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to tokenize text\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'],\n",
    "                     truncation=True,\n",
    "                     max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780f248764f04805a9cdaebe2e4554a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/982700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "tokenized_shopping = pretrain_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['text', 'title', 'query']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=128\n",
    "chunk_size=128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7204e32322489abcfbe4ba400a1c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/982700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply group_texts function over dataset\n",
    "lm_dataset = tokenized_shopping.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 158160\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dry eyes advanced relief 10 ml</s><s>cover stock paper bright color paper colorful cardstock  85’’ x 11’’ letter paper size 65lb cover </s><s>fashion photography rare vintage american eccentric fashion photographer  cameras</s><s>ice hockey masks  shields hockey shield  replacement lenses  prizm clear</s><s>adobe certification adobe photoshop dasturi paperback</s><s>space fleet science fiction voyaging volume one the plague star a graphic novel book</s><s>kids microscopes discovery kids discovery mindblown microscope set 48piece with durable metal </s><s>ceiling fan pull chain ornaments space imaginext ion crab'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode ids to ensure data can be recovered\n",
    "tokenizer.decode(lm_dataset[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dry eyes advanced relief 10 ml</s><s>cover stock paper bright color paper colorful cardstock  85’’ x 11’’ letter paper size 65lb cover </s><s>fashion photography rare vintage american eccentric fashion photographer  cameras</s><s>ice hockey masks  shields hockey shield  replacement lenses  prizm clear</s><s>adobe certification adobe photoshop dasturi paperback</s><s>space fleet science fiction voyaging volume one the plague star a graphic novel book</s><s>kids microscopes discovery kids discovery mindblown microscope set 48piece with durable metal </s><s>ceiling fan pull chain ornaments space imaginext ion crab'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode labels to ensure they match ids\n",
    "tokenizer.decode(lm_dataset[1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically pad sentences to the longest length of batch\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize data collator to randomly mask some of the tokens in each batch (15%)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> <s>adaptive drinking straws perishig<mask> straw<mask>  cleaning brush jeepers creepers  black glitter</s><s><mask> boys outerwear jackets carters inspired boys hood<mask> sweater jacket sizeotive months beige fleece</s><s>mixed drink<mask> sets mism<mask> colors wine <mask> glasses collection set of 4 vintage</s><s>rod end bearings aurora mg8 rod<mask> bearing 12</s><s>hanukkah music a hanukkah suite</s><s>climbing active protection hardware metolius<mask>  mountaineering equipment ultralight power camIDENT</s><s>dry eye relief products lot of<mask> idrop vet plus for moderate'\n",
      "\n",
      "'>>>  dry eyes advanced relief<mask> ml</s><s><mask> stock paper bright color paper<mask> cardstock  85�<mask>’ x 11’’ letter paper size<mask>lb cover </s><s>ption photography rare vintage american eccentric fashion<mask>  cameras</s><s><mask> hockey masks  shields hockey shield  replacement lenses  prizm clear</s><s>adobeATHER<mask>obe patriarchoshop d<mask>uri paperback</s><s>space fleet science fiction voyaging volume one the plague<mask> a graphic novel book</s><s>kids microscopes discovery kids discovery mindblown<mask><mask> 48piece with durable<mask> Veterans</s><s>ceiling fan pull chain ornaments space imaginext ion crab'\n"
     ]
    }
   ],
   "source": [
    "# Test masking to ensure it worked\n",
    "samples = [lm_dataset[i] for i in range(2)]\n",
    "\n",
    "for chunk in data_collator(samples)['input_ids']:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 110712\n",
      "Test set size: 47448\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into 70% train and 30% test\n",
    "train_test_split = lm_dataset.train_test_split(test_size=0.30)\n",
    "\n",
    "# Access the train and test splits\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Display the number of samples in each set\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 110712\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 47448\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "# Pull pre-trained model and tokenizer from huggingface hub\n",
    "model = RobertaForCausalLM.from_pretrained(\"twburns/group12_mlm_Distilled_Roberta\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('twburns/group12_mlm_Distilled_Roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # The eval_pred object contains predictions and label_ids (true labels).\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    loss = eval_pred.metrics[\"eval_loss\"]  # Get the evaluation loss\n",
    "    perplexity = math.exp(loss)  # Compute perplexity from loss\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_args = TrainingArguments(\\n    output_dir=\"group12_mlm_Distilled_Roberta\",\\n    eval_strategy=\"epoch\",\\n    learning_rate=2e-5,\\n    num_train_epochs=5,\\n    save_steps=100_000,\\n    save_total_limit=2,\\n    weight_decay=0.01, \\n    remove_unused_columns = False,\\n    per_device_train_batch_size=16,\\n    per_device_eval_batch_size=16\\n    #push_to_hub=True,\\n)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    compute_metrics=compute_metrics,\\n    train_dataset=train_dataset,\\n    eval_dataset=test_dataset,\\n    data_collator=data_collator,\\n    tokenizer=tokenizer,\\n)'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''training_args = TrainingArguments(\n",
    "    output_dir=\"group12_mlm_Distilled_Roberta\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=100_000,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01, \n",
    "    remove_unused_columns = False,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#eval_results = trainer.evaluate()\n",
    "\n",
    "#print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForCausalLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to GPU (if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Group 12 Pretrained Distilled Roberta number of parameters: 82M'\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> Group 12 Pretrained Distilled Roberta number of parameters: {round(distilbert_num_parameters)}M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasburns/opt/miniconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"twburns/group12_mlm_Distilled_Roberta\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"twburns/group12_mlm_Distilled_Roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Adjustable Car <mask> Holder for Easy Navigation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Adjustable Car holder Holder for Easy Navigation'\n",
      "'>>> Adjustable Car Holder Holder for Easy Navigation'\n",
      "'>>> Adjustable Car holders Holder for Easy Navigation'\n",
      "'>>> Adjustable Car lock Holder for Easy Navigation'\n",
      "'>>> Adjustable Car opener Holder for Easy Navigation'\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Adjustable Car holder Holder for Easy Navigation'\n",
      "'>>> Adjustable Car Holder Holder for Easy Navigation'\n",
      "'>>> Adjustable Car holders Holder for Easy Navigation'\n",
      "'>>> Adjustable Car lock Holder for Easy Navigation'\n",
      "'>>> Adjustable Car opener Holder for Easy Navigation'\n",
      "'>>> Adjustable Car clip Holder for Easy Navigation'\n",
      "'>>> Adjustable Car rack Holder for Easy Navigation'\n",
      "'>>> Adjustable Car mount Holder for Easy Navigation'\n",
      "'>>> Adjustable Car cover Holder for Easy Navigation'\n",
      "'>>> Adjustable Car handle Holder for Easy Navigation'\n"
     ]
    }
   ],
   "source": [
    "# Function to filter out duplicates in the final output\n",
    "def get_unique_replacements(text, top_tokens):\n",
    "    seen_replacements = set()\n",
    "    results = []\n",
    "\n",
    "    for token in top_tokens:\n",
    "        decoded_token = tokenizer.decode([token]).strip()\n",
    "        # Check if this replacement has already been used\n",
    "        if decoded_token not in seen_replacements:\n",
    "            seen_replacements.add(decoded_token)\n",
    "            # Replace the mask token and store the result\n",
    "            result = text.replace(tokenizer.mask_token, decoded_token)\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Sample the top tokens and generate unique replacements\n",
    "top_5_tokens = torch.topk(mask_token_logits, 10, dim=1).indices[0].tolist()\n",
    "unique_replacements = get_unique_replacements(text, top_5_tokens)\n",
    "\n",
    "# Print unique replacements\n",
    "for replacement in unique_replacements:\n",
    "    print(f\"'>>> {replacement}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2B5269; color: white; padding: 10px; \">\n",
    "<h1> Testing on Amazon Dataset <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_path = os.path.join('..', 'data', 'shopping_queries_dataset_examples.parquet')\n",
    "products_path = os.path.join('..', 'data', 'shopping_queries_dataset_products.parquet')\n",
    "sources_path = os.path.join('..', 'data', 'shopping_queries_dataset_sources.csv')\n",
    "\n",
    "examples = pd.read_parquet(examples_path)\n",
    "products = pd.read_parquet(products_path)\n",
    "sources = pd.read_csv(sources_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "# Pull pre-trained model and tokenizer from huggingface hub\n",
    "model = RobertaForCausalLM.from_pretrained(\"twburns/group12_mlm_Distilled_Roberta\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('twburns/group12_mlm_Distilled_Roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>small_version</th>\n",
       "      <th>large_version</th>\n",
       "      <th>split</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B000MOO21W</td>\n",
       "      <td>us</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceil...</td>\n",
       "      <td>None</td>\n",
       "      <td>WhisperCeiling fans feature a totally enclosed...</td>\n",
       "      <td>Panasonic</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07X3Y6B1V</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Homewerks 7141-80 Bathroom Fan Integrated LED ...</td>\n",
       "      <td>None</td>\n",
       "      <td>OUTSTANDING PERFORMANCE: This Homewerk's bath ...</td>\n",
       "      <td>Homewerks</td>\n",
       "      <td>80 CFM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07WDM7MQQ</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Homewerks 7140-80 Bathroom Fan Ceiling Mount E...</td>\n",
       "      <td>None</td>\n",
       "      <td>OUTSTANDING PERFORMANCE: This Homewerk's bath ...</td>\n",
       "      <td>Homewerks</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07RH6Z8KW</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Delta Electronics RAD80L BreezRadiance 80 CFM ...</td>\n",
       "      <td>This pre-owned or refurbished product has been...</td>\n",
       "      <td>Quiet operation at 1.5 sones\\nBuilt-in thermos...</td>\n",
       "      <td>DELTA ELECTRONICS (AMERICAS) LTD.</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07QJ7WYFQ</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Panasonic FV-08VRE2 Ventilation Fan with Reces...</td>\n",
       "      <td>None</td>\n",
       "      <td>The design solution for Fan/light combinations...</td>\n",
       "      <td>Panasonic</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id           query  query_id  product_id product_locale esci_label  \\\n",
       "0           0   revent 80 cfm         0  B000MOO21W             us          I   \n",
       "1           1   revent 80 cfm         0  B07X3Y6B1V             us          E   \n",
       "2           2   revent 80 cfm         0  B07WDM7MQQ             us          E   \n",
       "3           3   revent 80 cfm         0  B07RH6Z8KW             us          E   \n",
       "4           4   revent 80 cfm         0  B07QJ7WYFQ             us          E   \n",
       "\n",
       "   small_version  large_version  split  \\\n",
       "0              0              1  train   \n",
       "1              0              1  train   \n",
       "2              0              1  train   \n",
       "3              0              1  train   \n",
       "4              0              1  train   \n",
       "\n",
       "                                       product_title  \\\n",
       "0  Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceil...   \n",
       "1  Homewerks 7141-80 Bathroom Fan Integrated LED ...   \n",
       "2  Homewerks 7140-80 Bathroom Fan Ceiling Mount E...   \n",
       "3  Delta Electronics RAD80L BreezRadiance 80 CFM ...   \n",
       "4  Panasonic FV-08VRE2 Ventilation Fan with Reces...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3  This pre-owned or refurbished product has been...   \n",
       "4                                               None   \n",
       "\n",
       "                                product_bullet_point  \\\n",
       "0  WhisperCeiling fans feature a totally enclosed...   \n",
       "1  OUTSTANDING PERFORMANCE: This Homewerk's bath ...   \n",
       "2  OUTSTANDING PERFORMANCE: This Homewerk's bath ...   \n",
       "3  Quiet operation at 1.5 sones\\nBuilt-in thermos...   \n",
       "4  The design solution for Fan/light combinations...   \n",
       "\n",
       "                       product_brand product_color  \n",
       "0                          Panasonic         White  \n",
       "1                          Homewerks        80 CFM  \n",
       "2                          Homewerks         White  \n",
       "3  DELTA ELECTRONICS (AMERICAS) LTD.         White  \n",
       "4                          Panasonic         White  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_products = pd.merge(\n",
    "    examples,\n",
    "    products,\n",
    "    how='left',\n",
    "    left_on=['product_locale','product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "examples_products = examples_products[examples_products['product_locale'] == 'us']\n",
    "\n",
    "examples_products.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random n number of queries\n",
    "n = 10  # Specify the number of random queries you want to sample\n",
    "random_queries = examples_products.sample(n=n, random_state=2000)  \n",
    "queries = random_queries['query'].tolist()\n",
    "\n",
    "# Sample 1,000 random products\n",
    "subset_product_titles = examples_products['product_title'].sample(n=1000, random_state=2000).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    subset_product_titles = examples_products['product_title'].sample(n=1000, random_state=2000).tolist()\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate predictions\n",
    "def generate_predictions(queries, product_titles, k, batch_size):\n",
    "    all_results = []  # This will hold the final results\n",
    "\n",
    "    # Process each query\n",
    "    for query in queries:\n",
    "        query_results = []  # Store results for the current query\n",
    "        seen_titles = set()  # Track seen product titles\n",
    "\n",
    "        # Prepare input texts for the current query in batches\n",
    "        for i in range(0, len(product_titles), batch_size):\n",
    "            batch_titles = product_titles[i:i + batch_size]\n",
    "            input_texts = [f\"{query} <mask> {title}\" for title in batch_titles]\n",
    "            inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "            # Move inputs to the appropriate device\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "            # Make sure to use the model in evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            # Process each title's predictions for the current batch\n",
    "            for j in range(len(batch_titles)):\n",
    "                mask_index = (inputs['input_ids'][j] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "                if mask_index.numel() == 0:  # Check if mask token was found\n",
    "                    print(f\"No mask token found for input: {input_texts[j]}\")\n",
    "                    continue  # Skip this input if no mask token was found\n",
    "                \n",
    "                mask_logits = logits[j, mask_index.item()]\n",
    "                \n",
    "                # Get the top_k predictions\n",
    "                top_k_indices = torch.topk(mask_logits, k).indices\n",
    "                predicted_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "                # Ensure the product title is unique for this query\n",
    "                product_title = batch_titles[j]\n",
    "                if product_title not in seen_titles:\n",
    "                    seen_titles.add(product_title)  # Mark this title as seen\n",
    "                    query_results.append({\n",
    "                        'query': query,\n",
    "                        'product_title': product_title,\n",
    "                        'predicted_tokens': predicted_tokens,\n",
    "                        'logits': mask_logits  # Store logits for sorting\n",
    "                    })\n",
    "        \n",
    "        # Sort query results based on the relevance (logits) and limit to top k unique results\n",
    "        query_results.sort(key=lambda x: x['logits'].max().item(), reverse=True)  # Sort by max logit\n",
    "        top_k_results = []  # To collect unique results for this query\n",
    "\n",
    "        for result in query_results:\n",
    "            if len(top_k_results) < k:  # Limit to k results\n",
    "                if result['product_title'] not in {r['product_title'] for r in top_k_results}:\n",
    "                    top_k_results.append(result)\n",
    "\n",
    "        # Append the unique results for this query to the overall results\n",
    "        all_results.extend(top_k_results[:k])  # Ensure only top k are taken\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    query  \\\n",
      "0                beach waver curling iron   \n",
      "1                beach waver curling iron   \n",
      "2                beach waver curling iron   \n",
      "3                beach waver curling iron   \n",
      "4                beach waver curling iron   \n",
      "5   long shelves for bedroom wall mounted   \n",
      "6   long shelves for bedroom wall mounted   \n",
      "7   long shelves for bedroom wall mounted   \n",
      "8   long shelves for bedroom wall mounted   \n",
      "9   long shelves for bedroom wall mounted   \n",
      "10                              key chain   \n",
      "11                              key chain   \n",
      "12                              key chain   \n",
      "13                              key chain   \n",
      "14                              key chain   \n",
      "15                                  spode   \n",
      "16                                  spode   \n",
      "17                                  spode   \n",
      "18                                  spode   \n",
      "19                                  spode   \n",
      "20     rae dunn christmas reindeer snacks   \n",
      "21     rae dunn christmas reindeer snacks   \n",
      "22     rae dunn christmas reindeer snacks   \n",
      "23     rae dunn christmas reindeer snacks   \n",
      "24     rae dunn christmas reindeer snacks   \n",
      "25                       hd original xbox   \n",
      "26                       hd original xbox   \n",
      "27                       hd original xbox   \n",
      "28                       hd original xbox   \n",
      "29                       hd original xbox   \n",
      "30                galaxy watch 46mm bands   \n",
      "31                galaxy watch 46mm bands   \n",
      "32                galaxy watch 46mm bands   \n",
      "33                galaxy watch 46mm bands   \n",
      "34                galaxy watch 46mm bands   \n",
      "35                            dog harness   \n",
      "36                            dog harness   \n",
      "37                            dog harness   \n",
      "38                            dog harness   \n",
      "39                            dog harness   \n",
      "40               pickled white wood stain   \n",
      "41               pickled white wood stain   \n",
      "42               pickled white wood stain   \n",
      "43               pickled white wood stain   \n",
      "44               pickled white wood stain   \n",
      "45                       well waterbottle   \n",
      "46                       well waterbottle   \n",
      "47                       well waterbottle   \n",
      "48                       well waterbottle   \n",
      "49                       well waterbottle   \n",
      "\n",
      "                                        product_title  \\\n",
      "0   6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "1   16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "2   13\" Vintage Style Solar Powered Flame Effect (...   \n",
      "3   100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "4   50 Pack BPA-Free Disposable Baby Food Freezer ...   \n",
      "5   6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "6   16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "7   13\" Vintage Style Solar Powered Flame Effect (...   \n",
      "8   100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "9   50 Pack BPA-Free Disposable Baby Food Freezer ...   \n",
      "10  6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "11  100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "12  16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "13  40 Pieces Wax Strips Hair Remover Wax Strip Wa...   \n",
      "14  50 Pack BPA-Free Disposable Baby Food Freezer ...   \n",
      "15  6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "16  100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "17  40 Pieces Wax Strips Hair Remover Wax Strip Wa...   \n",
      "18  16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "19  50 Pack BPA-Free Disposable Baby Food Freezer ...   \n",
      "20  16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "21  100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "22  6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "23  50 Pack BPA-Free Disposable Baby Food Freezer ...   \n",
      "24  Glutathione Whitening Pills - 2000mcg Glutathi...   \n",
      "25  6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "26  16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "27  100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "28  10 Pieces Silicone Bookmark Resin Mold DIY Boo...   \n",
      "29  Balsamic Vinegar (Barrel Aged) High Density 1....   \n",
      "30  16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "31  Balsamic Vinegar (Barrel Aged) High Density 1....   \n",
      "32  6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "33  13\" Vintage Style Solar Powered Flame Effect (...   \n",
      "34  Glutathione Whitening Pills - 2000mcg Glutathi...   \n",
      "35  6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "36  100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "37  16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "38  40 Pieces Wax Strips Hair Remover Wax Strip Wa...   \n",
      "39  50 Pack BPA-Free Disposable Baby Food Freezer ...   \n",
      "40  6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "41  16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "42  50 Pack BPA-Free Disposable Baby Food Freezer ...   \n",
      "43  Balsamic Vinegar (Barrel Aged) High Density 1....   \n",
      "44  100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "45  6mm Color Changing Ring Mood Emotion Temperatu...   \n",
      "46  16' x 24' Guest House / Garden Storage Shed wi...   \n",
      "47  100ft Expandable Garden Hose Flexible Water Ho...   \n",
      "48  50 Pack BPA-Free Disposable Baby Food Freezer ...   \n",
      "49  13\" Vintage Style Solar Powered Flame Effect (...   \n",
      "\n",
      "                predicted_tokens  \\\n",
      "0            [Ġ6, 6, Ġ5, Ġ7, Ġ8]   \n",
      "1       [Ġ16, 16, Ġ17, Ġ24, Ġ14]   \n",
      "2       [Ġ13, 13, Ġ14, Ġ12, Ġ27]   \n",
      "3      [Ġ100, 100, Ġ1, Ġ50, Ġ10]   \n",
      "4       [Ġ50, Ġ25, Ġ5, 50, Ġ100]   \n",
      "5            [Ġ6, Ġ5, 6, Ġ7, Ġ8]   \n",
      "6       [Ġ16, 16, Ġ24, Ġ17, Ġ14]   \n",
      "7       [Ġ13, 13, Ġ14, Ġ12, Ġ15]   \n",
      "8   [Ġ100, Ġ50, 100, Ġ150, Ġ200]   \n",
      "9        [Ġ50, Ġ25, Ġ5, Ġ48, 50]   \n",
      "10           [Ġ6, Ġ5, Ġ7, 6, Ġ8]   \n",
      "11    [Ġ100, Ġ50, Ġ10, Ġ1, Ġ200]   \n",
      "12      [Ġ16, 16, Ġ17, Ġ24, Ġ14]   \n",
      "13     [Ġ40, Ġ20, Ġ60, Ġ10, Ġ50]   \n",
      "14     [Ġ50, Ġ100, Ġ25, Ġ5, Ġ60]   \n",
      "15           [Ġ6, Ġ5, Ġ7, 6, Ġ8]   \n",
      "16     [Ġ100, Ġ50, Ġ1, Ġ10, 100]   \n",
      "17     [Ġ40, Ġ60, Ġ20, Ġ50, Ġ10]   \n",
      "18      [Ġ16, Ġ17, Ġ24, Ġ14, 16]   \n",
      "19     [Ġ50, Ġ100, Ġ60, Ġ5, Ġ30]   \n",
      "20      [Ġ16, 16, Ġ17, Ġ24, Ġ14]   \n",
      "21     [Ġ100, 100, Ġ50, Ġ10, Ġ1]   \n",
      "22           [Ġ6, Ġ5, 6, Ġ7, Ġ4]   \n",
      "23     [Ġ50, 50, Ġ100, Ġ30, Ġ25]   \n",
      "24       [ĠGl, gl, Ġgl, Gl, ogl]   \n",
      "25           [Ġ6, Ġ5, 6, Ġ7, Ġ8]   \n",
      "26      [Ġ16, Ġ17, 16, Ġ32, Ġ24]   \n",
      "27   [Ġ100, Ġ50, 100, Ġ10, Ġ200]   \n",
      "28        [Ġ10, Ġ20, Ġ5, 10, Ġ8]   \n",
      "29         [Ġb, b, Ġc, Ġhim, Ġs]   \n",
      "30      [Ġ16, 16, Ġ17, Ġ24, Ġ18]   \n",
      "31         [Ġb, b, Ġhim, Ġc, Ġs]   \n",
      "32           [Ġ6, Ġ5, 6, Ġ4, Ġ7]   \n",
      "33      [Ġ13, 13, Ġ12, Ġ14, Ġ26]   \n",
      "34       [ĠGl, gl, Ġgl, Gl, igl]   \n",
      "35           [Ġ6, Ġ5, Ġ7, Ġ8, 6]   \n",
      "36    [Ġ100, Ġ50, Ġ10, Ġ1, Ġ200]   \n",
      "37      [Ġ16, Ġ17, Ġ24, 16, Ġ14]   \n",
      "38     [Ġ40, Ġ60, Ġ20, Ġ50, Ġ10]   \n",
      "39     [Ġ50, Ġ100, Ġ25, Ġ5, Ġ60]   \n",
      "40           [Ġ6, Ġ5, 6, Ġ7, Ġ8]   \n",
      "41      [Ġ16, 16, Ġ24, Ġ17, Ġ12]   \n",
      "42      [Ġ50, Ġ100, Ġ5, Ġ25, 50]   \n",
      "43        [Ġb, b, Ġhim, Ġc, Ġbr]   \n",
      "44     [Ġ100, 100, Ġ1, Ġ50, Ġ10]   \n",
      "45           [Ġ6, Ġ5, 6, Ġ7, Ġ8]   \n",
      "46      [Ġ16, 16, Ġ17, Ġ24, Ġ14]   \n",
      "47     [Ġ100, Ġ50, Ġ1, Ġ10, 100]   \n",
      "48     [Ġ50, Ġ100, Ġ60, Ġ5, Ġ25]   \n",
      "49      [Ġ13, Ġ14, 13, Ġ12, Ġ15]   \n",
      "\n",
      "                                               logits  \n",
      "0   [tensor(3.2846), tensor(-3.3154), tensor(4.037...  \n",
      "1   [tensor(2.9934), tensor(-2.6003), tensor(5.087...  \n",
      "2   [tensor(1.3158), tensor(-3.5350), tensor(3.650...  \n",
      "3   [tensor(5.6912), tensor(-3.0291), tensor(5.873...  \n",
      "4   [tensor(1.9120), tensor(-4.3044), tensor(4.224...  \n",
      "5   [tensor(2.9430), tensor(-3.8144), tensor(5.427...  \n",
      "6   [tensor(3.8598), tensor(-1.6749), tensor(6.684...  \n",
      "7   [tensor(1.4390), tensor(-3.3427), tensor(4.950...  \n",
      "8   [tensor(7.0192), tensor(-2.9416), tensor(7.860...  \n",
      "9   [tensor(2.5834), tensor(-4.4404), tensor(5.796...  \n",
      "10  [tensor(1.8907), tensor(-4.4913), tensor(4.165...  \n",
      "11  [tensor(3.8086), tensor(-4.0131), tensor(5.939...  \n",
      "12  [tensor(1.0109), tensor(-3.7912), tensor(4.496...  \n",
      "13  [tensor(1.0894), tensor(-3.5187), tensor(6.120...  \n",
      "14  [tensor(0.1699), tensor(-4.6931), tensor(3.823...  \n",
      "15  [tensor(2.2380), tensor(-4.3863), tensor(4.296...  \n",
      "16  [tensor(4.7140), tensor(-4.0405), tensor(6.532...  \n",
      "17  [tensor(2.4320), tensor(-3.1975), tensor(6.479...  \n",
      "18  [tensor(0.9929), tensor(-3.9984), tensor(4.300...  \n",
      "19  [tensor(0.6046), tensor(-4.4805), tensor(4.489...  \n",
      "20  [tensor(0.2318), tensor(-4.1327), tensor(3.379...  \n",
      "21  [tensor(1.7420), tensor(-4.3595), tensor(4.326...  \n",
      "22  [tensor(0.0043), tensor(-4.8084), tensor(2.427...  \n",
      "23  [tensor(-0.6381), tensor(-4.7226), tensor(3.50...  \n",
      "24  [tensor(2.8780), tensor(-4.9002), tensor(6.076...  \n",
      "25  [tensor(2.7683), tensor(-3.4471), tensor(4.905...  \n",
      "26  [tensor(0.8568), tensor(-3.5372), tensor(2.520...  \n",
      "27  [tensor(4.4326), tensor(-3.6353), tensor(5.502...  \n",
      "28  [tensor(1.4939), tensor(-3.5265), tensor(3.793...  \n",
      "29  [tensor(1.6872), tensor(-3.1782), tensor(8.374...  \n",
      "30  [tensor(1.3991), tensor(-4.1903), tensor(3.114...  \n",
      "31  [tensor(1.0965), tensor(-4.2094), tensor(6.202...  \n",
      "32  [tensor(1.0172), tensor(-5.3123), tensor(3.145...  \n",
      "33  [tensor(0.0783), tensor(-4.9356), tensor(1.670...  \n",
      "34  [tensor(2.6967), tensor(-5.1122), tensor(5.820...  \n",
      "35  [tensor(0.8381), tensor(-4.3837), tensor(3.856...  \n",
      "36  [tensor(3.6808), tensor(-3.9176), tensor(5.670...  \n",
      "37  [tensor(0.7122), tensor(-3.6925), tensor(3.675...  \n",
      "38  [tensor(1.5374), tensor(-3.1942), tensor(6.479...  \n",
      "39  [tensor(-0.5609), tensor(-4.8012), tensor(4.19...  \n",
      "40  [tensor(3.7941), tensor(-3.4664), tensor(4.539...  \n",
      "41  [tensor(3.1663), tensor(-2.9392), tensor(4.770...  \n",
      "42  [tensor(1.5237), tensor(-4.1525), tensor(4.903...  \n",
      "43  [tensor(2.7560), tensor(-2.9698), tensor(8.128...  \n",
      "44  [tensor(4.7733), tensor(-3.4483), tensor(5.557...  \n",
      "45  [tensor(2.3046), tensor(-4.1823), tensor(3.601...  \n",
      "46  [tensor(2.0985), tensor(-3.2927), tensor(4.312...  \n",
      "47  [tensor(4.7566), tensor(-3.5494), tensor(5.647...  \n",
      "48  [tensor(1.3158), tensor(-4.1662), tensor(4.444...  \n",
      "49  [tensor(1.2266), tensor(-4.0373), tensor(3.346...  \n"
     ]
    }
   ],
   "source": [
    "# Test the model on the sampled queries\n",
    "# Return top 5 results\n",
    "k = 5\n",
    "batch_size = 16\n",
    "all_results = []\n",
    "\n",
    "all_results = generate_predictions(queries, subset_product_titles, k, batch_size)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2B5269; color: white; padding: 10px; \">\n",
    "<h1> Fine-Tuning <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'title', 'text'],\n",
       "    num_rows: 982700\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'title', 'text'],\n",
      "        num_rows: 687890\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'title', 'text'],\n",
      "        num_rows: 294810\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split pretrained dataset into 70% train and 30% test\n",
    "train_test_split = pretrain_dataset.train_test_split(test_size=0.30)\n",
    "\n",
    "# Access the train and test splits\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Display the number of samples in each set\n",
    "print(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Text: baseball  softball pitching machines sports attack hack attack baseball pitching machine 1001100 with extended legs'\n",
      "\n",
      "'>>> Text: climbing slings  runners 1 nylon runner sling'\n",
      "\n",
      "'>>> Text: contact grills nexgrill 4burner propane gas grill in black with side burner and stainless steel '\n"
     ]
    }
   ],
   "source": [
    "sample = train_test_split[\"train\"].shuffle(seed=2006).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Text: {row['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a71147d0ec44cd918c14b37fda0c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/687890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6c96fd43ce439984840352d243f768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/294810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 687890\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 294810\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_dataset = train_test_split.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"query\", \"title\"]\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Slicing produces a list of lists for each feature\\ntokenized_samples = tokenized_dataset[\"train\"]\\n\\nfor idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\\n    print(f\"\\'>>> Review {idx} length: {len(sample)}\\'\")'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_dataset[\"train\"]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'concatenated_examples = {\\n    k: sum(tokenized_samples[k], []) for k in tokenized_samples.column_names\\n}\\ntotal_length = len(concatenated_examples[\"input_ids\"])\\nprint(f\"\\'>>> Concatenated reviews length: {total_length}\\'\")'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all tokenized samples and print total length\n",
    "'''concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.column_names\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chunks = {\\n    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\\n    for k, t in concatenated_examples.items()\\n}\\n\\nfor chunk in chunks[\"input_ids\"]:\\n    print(f\"\\'>>> Chunk length: {len(chunk)}\\'\")'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cc076229044e7c8609fc2c7ec97b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/687890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b3a1f905e942b1b1635707d2795e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/294810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 110732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 47427\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> <s>baby<mask> protection<mask><mask><mask> baby noise canceling headphones<mask> protection<mask><mask><mask> up to</s><s>cloth diaper sprayers znts bidet<mask><mask> for toilet handheld cloth diaper sprayer 40650030</s><s>bottled  canned coffee<mask> ucc coffee with milk original blend  113<mask><mask></s><s>fishing<mask> sets 134 pcs fishing tool kit fishing<mask><mask> equipment fishing pliers kit fish hook</s><s>automotive replacement heater<mask><mask> ac  heater relays  hd truck mei1246</s><s><mask><mask> fu <mask><mask> chi uniform bottoms<mask> martial arts tai<mask> trousers<mask><mask> le'\n",
      "\n",
      "'>>> e kung fu wing chun pants</s><s>toy kitchen sets 2pieces wooden kids kitchen playset with light and sound set shop and smile</s><s>xbox 360 game keyboards turtle beach<mask><mask> keyboard</s><s>kids  baby door hangers baby do not<mask>  teal<mask> hanger</s><s>kayak  canoe<mask> racks field  stream deluxe kayak<mask> kit x5 <mask><mask> canoe carrier kit x1</s><s><mask><mask><mask><mask> parts laars spring bypass white 125175 pool<mask> <mask>0061400</s><s>music business used preowned this business of music a practical guide to the music industry </s><s>'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_dataset[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "# Downsize sample for faster training\n",
    "downsampled_dataset = lm_dataset[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=2006\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "model_checkpoint = \"group12_mlm_Distilled_Roberta\"\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-google\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=False,\n",
    "    no_cuda=True,\n",
    "    logging_steps=logging_steps,\n",
    "    #use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column name ['word_ids'] not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [103]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m downsampled_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdownsampled_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m downsampled_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      3\u001b[0m     insert_random_mask,\n\u001b[1;32m      4\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39mdownsampled_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m eval_dataset\u001b[38;5;241m.\u001b[39mrename_columns(\n\u001b[1;32m      8\u001b[0m     {\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     }\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/dataset_dict.py:364\u001b[0m, in \u001b[0;36mDatasetDict.remove_columns\u001b[0;34m(self, column_names)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: dataset\u001b[38;5;241m.\u001b[39mremove_columns(column_names\u001b[38;5;241m=\u001b[39mcolumn_names) \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/dataset_dict.py:364\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:2146\u001b[0m, in \u001b[0;36mDataset.remove_columns\u001b[0;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[1;32m   2144\u001b[0m missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 2146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2147\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2149\u001b[0m     )\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[1;32m   2152\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures[column_name]\n",
      "\u001b[0;31mValueError\u001b[0m: Column name ['word_ids'] not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']"
     ]
    }
   ],
   "source": [
    "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "# initialize Adam optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 5\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twburns/group12_mlm_Distilled_Roberta-finetuned-google'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"group12_mlm_Distilled_Roberta-finetuned-google\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasburns/opt/miniconda3/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "python(3609) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(3610) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(3614) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(3631) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(3632) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(3633) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Users/thomasburns/Documents/Repos/esci-shopping-queries/pre-training/group12_mlm_Distilled_Roberta-finetuned-google is already a clone of https://huggingface.co/twburns/group12_mlm_Distilled_Roberta-finetuned-google. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "python(3634) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(3635) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(3636) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(3637) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = model_name\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear any cached memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17482abcdc8640fab35512416f900bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     13\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m---> 15\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/accelerate/optimizer.py:172\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/optim/adamw.py:227\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         state_steps,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/optim/adamw.py:767\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 767\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/optim/adamw.py:434\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to the MPS device\n",
    "        batch = {k: v.to('mps') for k, v in batch.items()}\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # Backward pass\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # Move batch to the MPS device\n",
    "        batch = {k: v.to(accelerator.device) for k, v in batch.items}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: mps:0\n",
      "Batch device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Batch device: {next(iter(batch.values())).device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
