{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 41517,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.036129778163162075,
      "grad_norm": 16.711294174194336,
      "learning_rate": 1.9759134812245587e-05,
      "loss": 4.1968,
      "step": 500
    },
    {
      "epoch": 0.07225955632632415,
      "grad_norm": 18.562870025634766,
      "learning_rate": 1.9518269624491172e-05,
      "loss": 3.9589,
      "step": 1000
    },
    {
      "epoch": 0.10838933448948623,
      "grad_norm": 19.64666748046875,
      "learning_rate": 1.927740443673676e-05,
      "loss": 3.8745,
      "step": 1500
    },
    {
      "epoch": 0.1445191126526483,
      "grad_norm": 20.31036949157715,
      "learning_rate": 1.9036539248982346e-05,
      "loss": 4.3226,
      "step": 2000
    },
    {
      "epoch": 0.1806488908158104,
      "grad_norm": 17.710758209228516,
      "learning_rate": 1.879567406122793e-05,
      "loss": 4.1998,
      "step": 2500
    },
    {
      "epoch": 0.21677866897897247,
      "grad_norm": 16.08925437927246,
      "learning_rate": 1.8554808873473517e-05,
      "loss": 4.1693,
      "step": 3000
    },
    {
      "epoch": 0.25290844714213456,
      "grad_norm": 20.27533721923828,
      "learning_rate": 1.8313943685719106e-05,
      "loss": 4.0905,
      "step": 3500
    },
    {
      "epoch": 0.2890382253052966,
      "grad_norm": 15.265824317932129,
      "learning_rate": 1.807307849796469e-05,
      "loss": 4.034,
      "step": 4000
    },
    {
      "epoch": 0.3251680034684587,
      "grad_norm": 16.939180374145508,
      "learning_rate": 1.7832213310210276e-05,
      "loss": 3.9847,
      "step": 4500
    },
    {
      "epoch": 0.3612977816316208,
      "grad_norm": 15.86705493927002,
      "learning_rate": 1.759134812245586e-05,
      "loss": 3.9498,
      "step": 5000
    },
    {
      "epoch": 0.39742755979478284,
      "grad_norm": 17.779207229614258,
      "learning_rate": 1.735048293470145e-05,
      "loss": 3.911,
      "step": 5500
    },
    {
      "epoch": 0.43355733795794493,
      "grad_norm": 18.57127571105957,
      "learning_rate": 1.7109617746947036e-05,
      "loss": 3.8817,
      "step": 6000
    },
    {
      "epoch": 0.46968711612110703,
      "grad_norm": 17.239158630371094,
      "learning_rate": 1.686875255919262e-05,
      "loss": 3.8834,
      "step": 6500
    },
    {
      "epoch": 0.5058168942842691,
      "grad_norm": 18.001487731933594,
      "learning_rate": 1.6627887371438206e-05,
      "loss": 3.8214,
      "step": 7000
    },
    {
      "epoch": 0.5419466724474312,
      "grad_norm": 15.932443618774414,
      "learning_rate": 1.6387022183683795e-05,
      "loss": 3.8112,
      "step": 7500
    },
    {
      "epoch": 0.5780764506105932,
      "grad_norm": 16.397544860839844,
      "learning_rate": 1.614615699592938e-05,
      "loss": 3.7953,
      "step": 8000
    },
    {
      "epoch": 0.6142062287737553,
      "grad_norm": 15.985462188720703,
      "learning_rate": 1.5905291808174966e-05,
      "loss": 3.7953,
      "step": 8500
    },
    {
      "epoch": 0.6503360069369174,
      "grad_norm": 15.837895393371582,
      "learning_rate": 1.566442662042055e-05,
      "loss": 3.7557,
      "step": 9000
    },
    {
      "epoch": 0.6864657851000795,
      "grad_norm": 15.148113250732422,
      "learning_rate": 1.542356143266614e-05,
      "loss": 3.7311,
      "step": 9500
    },
    {
      "epoch": 0.7225955632632416,
      "grad_norm": 15.62695598602295,
      "learning_rate": 1.5182696244911723e-05,
      "loss": 3.7126,
      "step": 10000
    },
    {
      "epoch": 0.7587253414264037,
      "grad_norm": 14.517873764038086,
      "learning_rate": 1.494183105715731e-05,
      "loss": 3.709,
      "step": 10500
    },
    {
      "epoch": 0.7948551195895657,
      "grad_norm": 16.206026077270508,
      "learning_rate": 1.4700965869402897e-05,
      "loss": 3.6868,
      "step": 11000
    },
    {
      "epoch": 0.8309848977527278,
      "grad_norm": 14.275611877441406,
      "learning_rate": 1.4460100681648484e-05,
      "loss": 3.6793,
      "step": 11500
    },
    {
      "epoch": 0.8671146759158899,
      "grad_norm": 16.253400802612305,
      "learning_rate": 1.4219235493894068e-05,
      "loss": 3.6759,
      "step": 12000
    },
    {
      "epoch": 0.903244454079052,
      "grad_norm": 15.012998580932617,
      "learning_rate": 1.3978370306139655e-05,
      "loss": 3.6229,
      "step": 12500
    },
    {
      "epoch": 0.9393742322422141,
      "grad_norm": 16.624143600463867,
      "learning_rate": 1.3737505118385242e-05,
      "loss": 3.6119,
      "step": 13000
    },
    {
      "epoch": 0.9755040104053762,
      "grad_norm": 13.783960342407227,
      "learning_rate": 1.3496639930630826e-05,
      "loss": 3.598,
      "step": 13500
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.3862102031707764,
      "eval_runtime": 1082.0683,
      "eval_samples_per_second": 43.849,
      "eval_steps_per_second": 5.481,
      "step": 13839
    },
    {
      "epoch": 1.0116337885685383,
      "grad_norm": 14.718216896057129,
      "learning_rate": 1.3255774742876413e-05,
      "loss": 3.584,
      "step": 14000
    },
    {
      "epoch": 1.0477635667317002,
      "grad_norm": 16.884166717529297,
      "learning_rate": 1.3014909555122e-05,
      "loss": 3.5462,
      "step": 14500
    },
    {
      "epoch": 1.0838933448948624,
      "grad_norm": 14.893051147460938,
      "learning_rate": 1.2774044367367587e-05,
      "loss": 3.5675,
      "step": 15000
    },
    {
      "epoch": 1.1200231230580244,
      "grad_norm": 14.510852813720703,
      "learning_rate": 1.253317917961317e-05,
      "loss": 3.564,
      "step": 15500
    },
    {
      "epoch": 1.1561529012211864,
      "grad_norm": 14.624218940734863,
      "learning_rate": 1.2292313991858757e-05,
      "loss": 3.5636,
      "step": 16000
    },
    {
      "epoch": 1.1922826793843486,
      "grad_norm": 14.783768653869629,
      "learning_rate": 1.2051448804104344e-05,
      "loss": 3.5214,
      "step": 16500
    },
    {
      "epoch": 1.2284124575475106,
      "grad_norm": 15.503861427307129,
      "learning_rate": 1.1810583616349931e-05,
      "loss": 3.5511,
      "step": 17000
    },
    {
      "epoch": 1.2645422357106728,
      "grad_norm": 17.761478424072266,
      "learning_rate": 1.1569718428595515e-05,
      "loss": 3.5354,
      "step": 17500
    },
    {
      "epoch": 1.3006720138738348,
      "grad_norm": 15.087806701660156,
      "learning_rate": 1.1328853240841102e-05,
      "loss": 3.5145,
      "step": 18000
    },
    {
      "epoch": 1.3368017920369968,
      "grad_norm": 17.430633544921875,
      "learning_rate": 1.1087988053086689e-05,
      "loss": 3.5037,
      "step": 18500
    },
    {
      "epoch": 1.372931570200159,
      "grad_norm": 16.044389724731445,
      "learning_rate": 1.0847122865332274e-05,
      "loss": 3.5167,
      "step": 19000
    },
    {
      "epoch": 1.409061348363321,
      "grad_norm": 14.82409381866455,
      "learning_rate": 1.060625767757786e-05,
      "loss": 3.4831,
      "step": 19500
    },
    {
      "epoch": 1.4451911265264832,
      "grad_norm": 16.162670135498047,
      "learning_rate": 1.0365392489823447e-05,
      "loss": 3.5001,
      "step": 20000
    },
    {
      "epoch": 1.4813209046896452,
      "grad_norm": 15.76314640045166,
      "learning_rate": 1.0124527302069034e-05,
      "loss": 3.4414,
      "step": 20500
    },
    {
      "epoch": 1.5174506828528074,
      "grad_norm": 14.452301025390625,
      "learning_rate": 9.883662114314619e-06,
      "loss": 3.4563,
      "step": 21000
    },
    {
      "epoch": 1.5535804610159694,
      "grad_norm": 15.4188871383667,
      "learning_rate": 9.642796926560206e-06,
      "loss": 3.4578,
      "step": 21500
    },
    {
      "epoch": 1.5897102391791313,
      "grad_norm": 17.477420806884766,
      "learning_rate": 9.401931738805791e-06,
      "loss": 3.4634,
      "step": 22000
    },
    {
      "epoch": 1.6258400173422936,
      "grad_norm": 15.651961326599121,
      "learning_rate": 9.161066551051378e-06,
      "loss": 3.4362,
      "step": 22500
    },
    {
      "epoch": 1.6619697955054558,
      "grad_norm": 14.76777458190918,
      "learning_rate": 8.920201363296964e-06,
      "loss": 3.4098,
      "step": 23000
    },
    {
      "epoch": 1.6980995736686175,
      "grad_norm": 15.876420021057129,
      "learning_rate": 8.67933617554255e-06,
      "loss": 3.4333,
      "step": 23500
    },
    {
      "epoch": 1.7342293518317797,
      "grad_norm": 16.058048248291016,
      "learning_rate": 8.438470987788136e-06,
      "loss": 3.4296,
      "step": 24000
    },
    {
      "epoch": 1.770359129994942,
      "grad_norm": 14.922087669372559,
      "learning_rate": 8.197605800033723e-06,
      "loss": 3.4158,
      "step": 24500
    },
    {
      "epoch": 1.806488908158104,
      "grad_norm": 14.846885681152344,
      "learning_rate": 7.956740612279308e-06,
      "loss": 3.4181,
      "step": 25000
    },
    {
      "epoch": 1.842618686321266,
      "grad_norm": 14.758955955505371,
      "learning_rate": 7.715875424524895e-06,
      "loss": 3.4109,
      "step": 25500
    },
    {
      "epoch": 1.8787484644844281,
      "grad_norm": 14.719018936157227,
      "learning_rate": 7.4750102367704805e-06,
      "loss": 3.4029,
      "step": 26000
    },
    {
      "epoch": 1.91487824264759,
      "grad_norm": 15.498228073120117,
      "learning_rate": 7.234145049016066e-06,
      "loss": 3.4027,
      "step": 26500
    },
    {
      "epoch": 1.951008020810752,
      "grad_norm": 14.649834632873535,
      "learning_rate": 6.993279861261653e-06,
      "loss": 3.3907,
      "step": 27000
    },
    {
      "epoch": 1.9871377989739143,
      "grad_norm": 15.143656730651855,
      "learning_rate": 6.752414673507238e-06,
      "loss": 3.3921,
      "step": 27500
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.2046124935150146,
      "eval_runtime": 1048.8457,
      "eval_samples_per_second": 45.238,
      "eval_steps_per_second": 5.655,
      "step": 27678
    },
    {
      "epoch": 2.0232675771370765,
      "grad_norm": 15.502958297729492,
      "learning_rate": 6.511549485752825e-06,
      "loss": 3.3776,
      "step": 28000
    },
    {
      "epoch": 2.0593973553002383,
      "grad_norm": 17.588146209716797,
      "learning_rate": 6.2706842979984105e-06,
      "loss": 3.3735,
      "step": 28500
    },
    {
      "epoch": 2.0955271334634005,
      "grad_norm": 16.347476959228516,
      "learning_rate": 6.0298191102439975e-06,
      "loss": 3.3633,
      "step": 29000
    },
    {
      "epoch": 2.1316569116265627,
      "grad_norm": 16.67450523376465,
      "learning_rate": 5.788953922489583e-06,
      "loss": 3.3605,
      "step": 29500
    },
    {
      "epoch": 2.167786689789725,
      "grad_norm": 12.588717460632324,
      "learning_rate": 5.54808873473517e-06,
      "loss": 3.3628,
      "step": 30000
    },
    {
      "epoch": 2.2039164679528866,
      "grad_norm": 16.424406051635742,
      "learning_rate": 5.307223546980755e-06,
      "loss": 3.365,
      "step": 30500
    },
    {
      "epoch": 2.240046246116049,
      "grad_norm": 16.057559967041016,
      "learning_rate": 5.066358359226341e-06,
      "loss": 3.3372,
      "step": 31000
    },
    {
      "epoch": 2.276176024279211,
      "grad_norm": 16.008466720581055,
      "learning_rate": 4.8254931714719275e-06,
      "loss": 3.3468,
      "step": 31500
    },
    {
      "epoch": 2.312305802442373,
      "grad_norm": 16.909706115722656,
      "learning_rate": 4.584627983717514e-06,
      "loss": 3.3367,
      "step": 32000
    },
    {
      "epoch": 2.348435580605535,
      "grad_norm": 14.576116561889648,
      "learning_rate": 4.3437627959631e-06,
      "loss": 3.329,
      "step": 32500
    },
    {
      "epoch": 2.3845653587686972,
      "grad_norm": 15.216219902038574,
      "learning_rate": 4.102897608208686e-06,
      "loss": 3.3336,
      "step": 33000
    },
    {
      "epoch": 2.420695136931859,
      "grad_norm": 15.189441680908203,
      "learning_rate": 3.862032420454272e-06,
      "loss": 3.3282,
      "step": 33500
    },
    {
      "epoch": 2.456824915095021,
      "grad_norm": 14.388864517211914,
      "learning_rate": 3.6211672326998583e-06,
      "loss": 3.3465,
      "step": 34000
    },
    {
      "epoch": 2.4929546932581834,
      "grad_norm": 15.488941192626953,
      "learning_rate": 3.3803020449454445e-06,
      "loss": 3.327,
      "step": 34500
    },
    {
      "epoch": 2.5290844714213456,
      "grad_norm": 15.510377883911133,
      "learning_rate": 3.1394368571910306e-06,
      "loss": 3.3383,
      "step": 35000
    },
    {
      "epoch": 2.5652142495845074,
      "grad_norm": 13.525903701782227,
      "learning_rate": 2.898571669436617e-06,
      "loss": 3.3431,
      "step": 35500
    },
    {
      "epoch": 2.6013440277476696,
      "grad_norm": 14.277168273925781,
      "learning_rate": 2.657706481682203e-06,
      "loss": 3.3279,
      "step": 36000
    },
    {
      "epoch": 2.637473805910832,
      "grad_norm": 13.59744930267334,
      "learning_rate": 2.4168412939277887e-06,
      "loss": 3.3128,
      "step": 36500
    },
    {
      "epoch": 2.6736035840739936,
      "grad_norm": 16.296981811523438,
      "learning_rate": 2.175976106173375e-06,
      "loss": 3.317,
      "step": 37000
    },
    {
      "epoch": 2.7097333622371558,
      "grad_norm": 14.337113380432129,
      "learning_rate": 1.935110918418961e-06,
      "loss": 3.2965,
      "step": 37500
    },
    {
      "epoch": 2.745863140400318,
      "grad_norm": 14.731841087341309,
      "learning_rate": 1.6942457306645472e-06,
      "loss": 3.3181,
      "step": 38000
    },
    {
      "epoch": 2.78199291856348,
      "grad_norm": 14.569457054138184,
      "learning_rate": 1.4533805429101334e-06,
      "loss": 3.3175,
      "step": 38500
    },
    {
      "epoch": 2.818122696726642,
      "grad_norm": 17.43198585510254,
      "learning_rate": 1.2125153551557195e-06,
      "loss": 3.2985,
      "step": 39000
    },
    {
      "epoch": 2.854252474889804,
      "grad_norm": 15.873424530029297,
      "learning_rate": 9.716501674013055e-07,
      "loss": 3.2891,
      "step": 39500
    },
    {
      "epoch": 2.8903822530529664,
      "grad_norm": 15.239155769348145,
      "learning_rate": 7.307849796468918e-07,
      "loss": 3.2881,
      "step": 40000
    },
    {
      "epoch": 2.926512031216128,
      "grad_norm": 15.357819557189941,
      "learning_rate": 4.899197918924778e-07,
      "loss": 3.3009,
      "step": 40500
    },
    {
      "epoch": 2.9626418093792903,
      "grad_norm": 18.941577911376953,
      "learning_rate": 2.4905460413806393e-07,
      "loss": 3.3069,
      "step": 41000
    },
    {
      "epoch": 2.9987715875424525,
      "grad_norm": 13.799875259399414,
      "learning_rate": 8.189416383650072e-09,
      "loss": 3.3433,
      "step": 41500
    }
  ],
  "logging_steps": 500,
  "max_steps": 41517,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1012119088560128e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
