{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/wma002/python3.14/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: torch in /home/wma002/python3.14/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /home/wma002/python3.14/lib/python3.11/site-packages (4.46.2)\n",
      "Requirement already satisfied: scikit-learn in /home/wma002/python3.14/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /home/wma002/python3.14/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: dask[dataframe] in /home/wma002/python3.14/lib/python3.11/site-packages (2024.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/wma002/python3.14/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/wma002/python3.14/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/wma002/python3.14/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wma002/python3.14/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/wma002/python3.14/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/wma002/python3.14/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click>=8.1 in /home/wma002/python3.14/lib/python3.11/site-packages (from dask[dataframe]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from dask[dataframe]) (3.1.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from dask[dataframe]) (1.4.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/wma002/python3.14/lib/python3.11/site-packages (from dask[dataframe]) (8.5.0)\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in /home/wma002/python3.14/lib/python3.11/site-packages (from dask[dataframe]) (1.1.16)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in /home/wma002/python3.14/lib/python3.11/site-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (18.0.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/wma002/python3.14/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.20.2)\n",
      "Requirement already satisfied: locket in /home/wma002/python3.14/lib/python3.11/site-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/python/anaconda-3.14/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas torch transformers scikit-learn numpy \"dask[dataframe]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2xEzpVZEdkgh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning the Git Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkheqPnBYk-j",
    "outputId": "493751df-b6a2-4ce3-de0c-492509de68d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'esci-shopping-queries' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sarahlawlis/esci-shopping-queries.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PREl8016skCo"
   },
   "source": [
    "### 1. Preprocessing/Preparation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JmZ_rWFiH2Y",
    "outputId": "96bbd6c2-63b6-4f8e-fc5a-c456b95329ab"
   },
   "outputs": [],
   "source": [
    "# Load Data and Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv-wLQ5wi_QD"
   },
   "outputs": [],
   "source": [
    "examples_path = os.path.join('..', 'Practicum_Code', 'esci-shopping-queries', 'data', 'shopping_queries_dataset_examples.parquet')\n",
    "products_path = os.path.join('..', 'Practicum_Code', 'esci-shopping-queries', 'data', 'shopping_queries_dataset_products.parquet')\n",
    "sources_path = os.path.join('..', 'Practicum_Code', 'esci-shopping-queries', 'data', 'shopping_queries_dataset_sources.csv')\n",
    "\n",
    "# Load the data with Dask\n",
    "examples = dd.read_parquet(examples_path)\n",
    "products = dd.read_parquet(products_path)\n",
    "sources = dd.read_csv(sources_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4OBRlNymA3Q"
   },
   "outputs": [],
   "source": [
    "# Merge the examples and products datasets\n",
    "examples_products = dd.merge(\n",
    "    examples,\n",
    "    products,\n",
    "    how='left',\n",
    "    left_on=['product_locale', 'product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "# Filter to only 'us' locale\n",
    "examples_products = examples_products[examples_products['product_locale'] == 'us']\n",
    "\n",
    "# Filter for large versions (task_2)\n",
    "task_2 = examples_products[examples_products['large_version'] == 1]\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {'E': 0, 'S': 1, 'C': 2, 'I': 3}\n",
    "\n",
    "# Map labels to integers using map_partitions with meta to specify output type\n",
    "task_2['encoded_labels'] = task_2['esci_label'].map_partitions(\n",
    "    lambda df: df.map(label_mapping),\n",
    "    meta=('encoded_labels', 'int32')\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "task_2_train = task_2[task_2['split'] == 'train']\n",
    "task_2_test = task_2[task_2['split'] == 'test']\n",
    "\n",
    "# For further computation or saving as a Pandas DataFrame, use .compute()\n",
    "task_2_train = task_2_train.compute()\n",
    "task_2_test = task_2_test.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDn6ZC6pp60R"
   },
   "outputs": [],
   "source": [
    "# Filter to only 'us'\n",
    "examples_products = examples_products[examples_products['product_locale'] == 'us']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ8aUbsH8F03"
   },
   "source": [
    "2. Fine Tune Sentence Tranformer without Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMKtAYFfO4Ng",
    "outputId": "2b584c0e-ff2d-4fa8-ef5a-9cf3865ed64e"
   },
   "outputs": [],
   "source": [
    "# Load DistilBERT tokenizer and model\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model_bert = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load DistilRoBERTa tokenizer and model\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "model_roberta = AutoModel.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBNhLSfSTQ0j"
   },
   "outputs": [],
   "source": [
    "# Step 1: Tokenize your data and identify domain-specific vocabulary\n",
    "def get_domain_specific_vocabulary(texts, tokenizer_bert, tokenizer_roberta):\n",
    "    domain_vocab_bert = set()\n",
    "    domain_vocab_roberta = set()\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize with DistilBERT tokenizer\n",
    "        tokens_bert = tokenizer_bert.tokenize(text)\n",
    "        for token in tokens_bert:\n",
    "            if token.startswith(\"##\"):  # Check for subword tokens\n",
    "                continue\n",
    "            if token not in tokenizer_bert.vocab:  # If token is not in the DistilBERT vocab\n",
    "                domain_vocab_bert.add(token)\n",
    "        \n",
    "        # Tokenize with DistilRoBERTa tokenizer\n",
    "        tokens_roberta = tokenizer_roberta.tokenize(text)\n",
    "        for token in tokens_roberta:\n",
    "            if token.startswith(\"Ä \"):  # Check for RoBERTa subword prefix\n",
    "                continue\n",
    "            if token not in tokenizer_roberta.vocab:  # If token is not in the DistilRoBERTa vocab\n",
    "                domain_vocab_roberta.add(token)\n",
    "    \n",
    "    # Find mismatches unique to each model and common mismatches\n",
    "    unique_to_bert = domain_vocab_bert - domain_vocab_roberta\n",
    "    unique_to_roberta = domain_vocab_roberta - domain_vocab_bert\n",
    "    common_mismatches = domain_vocab_bert & domain_vocab_roberta\n",
    "    \n",
    "    return {\n",
    "        \"unique_to_bert\": list(unique_to_bert),\n",
    "        \"unique_to_roberta\": list(unique_to_roberta),\n",
    "        \"common_mismatches\": list(common_mismatches)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "5lrQ4bOyTZXl",
    "outputId": "03f85230-3838-4512-be86-f78659c4f100"
   },
   "outputs": [],
   "source": [
    "# Compute the examples_products DataFrame to perform Pandas operations\n",
    "examples_products_pd = examples_products.compute()\n",
    "\n",
    "# Collect texts from your merged dataframe\n",
    "texts = examples_products_pd['product_title'].fillna(\"\").tolist()\n",
    "\n",
    "# Identify domain-specific vocabulary mismatches for both tokenizers\n",
    "domain_vocab_mismatches = get_domain_specific_vocabulary(texts, tokenizer_bert, tokenizer_roberta)\n",
    "\n",
    "# Inspect mismatched vocabulary\n",
    "print(\"Unique to DistilBERT:\", domain_vocab_mismatches[\"unique_to_bert\"][:10])\n",
    "print(\"Unique to DistilRoBERTa:\", domain_vocab_mismatches[\"unique_to_roberta\"][:10])\n",
    "print(\"Common mismatches:\", domain_vocab_mismatches[\"common_mismatches\"][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "WqQUbT5bTl_W",
    "outputId": "5aa830f3-9711-478f-9541-ca5f86024c99"
   },
   "outputs": [],
   "source": [
    "def get_embeddings_for_vocab(vocab, tokenizer, model):\n",
    "    model.eval()\n",
    "    embeddings = {}\n",
    "    with torch.no_grad():\n",
    "        for word in vocab:\n",
    "            # Tokenize and convert to tensor\n",
    "            inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            # Use the [CLS] token representation as the embedding\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0).numpy()\n",
    "            embeddings[word] = cls_embedding\n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings for unique and common mismatches with both models\n",
    "domain_vocab_mismatches = get_domain_specific_vocabulary(texts, tokenizer_bert, tokenizer_roberta)\n",
    "\n",
    "# Embeddings for terms unique to DistilBERT\n",
    "bert_embeddings = get_embeddings_for_vocab(domain_vocab_mismatches[\"unique_to_bert\"], tokenizer_bert, model_bert)\n",
    "\n",
    "# Embeddings for terms unique to DistilRoBERTa\n",
    "roberta_embeddings = get_embeddings_for_vocab(domain_vocab_mismatches[\"unique_to_roberta\"], tokenizer_roberta, model_roberta)\n",
    "\n",
    "# Embeddings for common mismatches (both models)\n",
    "common_embeddings_bert = get_embeddings_for_vocab(domain_vocab_mismatches[\"common_mismatches\"], tokenizer_bert, model_bert)\n",
    "common_embeddings_roberta = get_embeddings_for_vocab(domain_vocab_mismatches[\"common_mismatches\"], tokenizer_roberta, model_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lwtzKC1TrjE"
   },
   "outputs": [],
   "source": [
    "# Step 3: Use these embeddings to enrich domain knowledge\n",
    "\n",
    "# Example: Check a few embeddings unique to DistilBERT\n",
    "print(\"Embeddings unique to DistilBERT:\")\n",
    "for word, embedding in list(bert_embeddings.items())[:5]:\n",
    "    print(f\"Word: {word}\\nEmbedding: {embedding[:10]}...\")  # Show the first 10 values\n",
    "\n",
    "# Example: Check a few embeddings unique to DistilRoBERTa\n",
    "print(\"\\nEmbeddings unique to DistilRoBERTa:\")\n",
    "for word, embedding in list(roberta_embeddings.items())[:5]:\n",
    "    print(f\"Word: {word}\\nEmbedding: {embedding[:10]}...\")  # Show the first 10 values\n",
    "\n",
    "# Example: Check a few embeddings for common mismatches in DistilBERT\n",
    "print(\"\\nCommon mismatches embeddings from DistilBERT:\")\n",
    "for word, embedding in list(common_embeddings_bert.items())[:5]:\n",
    "    print(f\"Word: {word}\\nEmbedding: {embedding[:10]}...\")  # Show the first 10 values\n",
    "\n",
    "# Example: Check a few embeddings for common mismatches in DistilRoBERTa\n",
    "print(\"\\nCommon mismatches embeddings from DistilRoBERTa:\")\n",
    "for word, embedding in list(common_embeddings_roberta.items())[:5]:\n",
    "    print(f\"Word: {word}\\nEmbedding: {embedding[:10]}...\")  # Show the first 10 values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
